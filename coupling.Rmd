---
title: "Spatial coupling in measles via fade-out survival analysis"
csl: ecology.csl
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    code_folding: hide
    dev: png
    df_print: paged
    highlight: tango
    number_sections: no
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
      smooth_scroll: yes
params:
  prefix: coupling
bibliography: gravity.bib
---

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta[1]{{\Delta}{#1}}
\newcommand\lik{\mathscr{L}}
\newcommand\loglik{\ell}

Produced with **R** version `r getRversion()`.

--------------------------

```{r knitr-opts,include=FALSE,cache=FALSE,purl=FALSE}
library(knitr)
opts_chunk$set(
  eval=TRUE,
  purl=FALSE,
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  cache.extra=rand_seed,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",params$prefix,"-"),
  cache.path=paste0("cache/",params$prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
)
```

# Preparations

```{r prelims,cache=FALSE,purl=TRUE}
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  knitr.kable.NA="",
  encoding="UTF-8",
  aakmisc.dbname="ewmeasles",
  aakmisc.remotehost="kinglab.eeb.lsa.umich.edu",
  aakmisc.user="gravity")

set.seed(594709947L)

library(grid)
library(plyr)
library(reshape2)
library(magrittr)
library(foreach)
library(iterators)
library(bbmle)
library(nloptr)
library(pomp)
library(ggplot2)
library(scales)
library(sp)
library(ncf)
theme_set(theme_bw())
```

## Preprocess the data.

Retrieve the data from the database.

```{r clean-data,purl=TRUE}
stew(file="clean_data.rda",{
  library(aakmisc)
  
  startTunnel()
  
  getQuery("select town,year,births,pop from demog where year>=1944 order by town,year") -> demog
  
  getQuery("select town,date,cases from measles where year>=1944 order by town,date") %>%
    mutate(year=as.integer(format(date+3,"%Y"))) %>%
    ddply(~town+year,mutate,week=seq_along(year),biweek=(week+1)%/%2) %>%
    subset(week<=52,select=-c(date,week)) %>%
    acast(town~year~biweek,value.var="cases",fun.aggregate=sum) %>%
    melt(varnames=c("town","year","biweek"),value.name="cases") %>%
    mutate(town=as.character(town)) %>%
    arrange(town,year,biweek) %>%
    join(demog,by=c("town","year")) %>%
    mutate(births=births/26) -> dat
  
  stopTunnel()
})
```

In the above:

1. Measles data is weekly; dates given correspond to Sunday.
1. Associate each week's data with the Weds. of that week.
1. Discard any 53rd weeks (1947,1952,1958,1964).
1. Aggregate by biweek (26 biweeks/yr).
1. Join with demographic data.
1. Scale birth rates to births/biweek.

## Fadeouts

```{r fadeout-plot,fig.height=4.4,purl=FALSE}
dat %>%
  ddply(~town,summarize,
        efrac=(sum(cases==0)+0.5)/(length(cases)+0.5),
        N=mean(pop)) -> fadeouts

lm(qlogis(p=efrac)~log(N),data=fadeouts,subset=efrac>0) -> fit
predict(fit,se.fit=TRUE) -> pfit

fadeouts %>%
  mutate(
    pred=plogis(q=pfit$fit),
    hi=plogis(q=pfit$fit+2*pfit$se.fit),
    lo=plogis(q=fit$fit-2*pfit$se.fit),
    residual=residuals(fit)
  ) -> fadeouts

fadeouts %>%
  ggplot(aes(x=N,y=efrac))+
  geom_point()+
  geom_line(aes(y=pred))+
  geom_ribbon(aes(ymin=lo,ymax=hi),color="grey50",alpha=0.3)+
  scale_x_log10()+
  labs(y="proportion of zeros")

fadeouts %>%
  ggplot(aes(x=N,y=efrac))+
  geom_point()+
  geom_line(aes(y=pred))+
  geom_ribbon(aes(ymin=lo,ymax=hi),color="grey50",alpha=0.3)+
  scale_y_continuous(trans=logit_trans())+
  scale_x_log10()+
  labs(y="proportion of zeros")

fadeouts %>%
  ggplot(aes(x=N,y=residual))+
  geom_point()+
  scale_x_log10()

fadeouts %>%
  ggplot(aes(x=pred,y=residual))+
  geom_point()+
  labs(x="fitted")

fadeouts %>%
  ggplot(aes(x=pred,y=abs(residual)))+
  geom_point()+
  labs(x="fitted",y=expression(group("|",residual,"|")))

plot(fit,which=1:5,ask=FALSE)
```

## Coordinates and distances

Now let's compute the distances between the cities.

```{r coords,purl=TRUE}
bake(file="coords.rds",{
  library(aakmisc)
  startTunnel()
  getQuery("select * from coords") %>% arrange(town) -> coords
  stopTunnel()
  coords
}) -> coords
```

```{r distances,purl=TRUE}
bake(file="distances.rds",{
  library(aakmisc)
  library(geosphere)
  
  distm(
    coords %>% subset(select=c(long,lat)),
    coords %>% subset(select=c(long,lat))) %>%
    matrix(nrow=nrow(coords),ncol=nrow(coords),
           dimnames=list(coords$town,coords$town))
}) -> distances
```

## Apply TSIR model to the individual towns.

```{r startDoMC,cache=FALSE,include=FALSE,purl=TRUE}
  library(doMC)
  registerDoMC()
```

### Susceptible reconstruction.

Smoothing spline regression of cumulative cases on cumulative births to estimate under-reporting and residuals.
Correct for under-reporting: `I = cases/ur`.

Regress cumulative cases on cumulative births to estimate under-reporting (ur) and susceptible depletion (`z`) `I` is estimated as `cases/ur`.

```{r under-reporting,purl=TRUE}
bake(file="under-reporting.rds",{
  foreach (d=dlply(dat,~town),
           .combine=rbind,.inorder=FALSE,
           .packages=c("plyr")
           ) %dopar% {
    cumbirths <- cumsum(d$births)
    cumcases <- cumsum(d$cases)
    fit <- smooth.spline(cumbirths,cumcases,df=2.5)
    mutate(d,
           ur=predict(fit,x=cumbirths,deriv=1)$y,
           I=cases/ur)
  } -> dat
}) -> dat
```

Now reconstruct the susceptibles (via the residuals `z`).

```{r susceptible-reconstruction,purl=TRUE}
bake(file="susc-reconst.rds",{
  foreach (d=dlply(dat,~town),
           .combine=rbind,.inorder=FALSE,
           .packages=c("plyr")) %dopar% {
             cumbirths <- cumsum(d$births)
             cuminc <- cumsum(d$I)
             fit <- smooth.spline(cumbirths,cuminc,df=2.5)
             mutate(d,z=-residuals(fit))
           } -> dat
}) -> dat
```

*Does the preceding scale the z variable appropriately?*
*I.e., does it take under-reporting into account?*

### Fit TSIR

First, set up the data matrix for the regression.
This involves lagging the I and z variables.
Population size is assumed constant at its median value.

```{r make-data-matrix,purl=TRUE}
bake(file="data-matrix.rds",{
  dat %>%
    ddply(~town,mutate,
          Ilag=c(NA,head(I,-1)),
          zlag=c(NA,head(z,-1)),
          Ps=median(pop),
          seas=factor(biweek,levels=1:26)) %>%
    ddply(~town,tail,-1) -> dat
}) -> dat
```

We'll start by estimating the mean susceptible fraction, assuming a single global value for this parameter.
Some towns have very high changes in susceptible fraction; exclude these.

```{r exclude-weird-towns,purl=TRUE}
bake(file="exclusions.rds",{
  dat %>%
    ddply(~town,summarize,
          maxfrac=max(-z/Ps)) %>%
    subset(maxfrac>0.03) %>%
    extract2("town") -> excludes
}) -> excludes

print(length(excludes))
```

Now profile on the fraction, $\sigma$, of susceptibles in the population.
This assumes a single, global value of $\sigma$.
**NOTE: the $\beta$ are here scaled by population size.**

```{r sigma-profile,purl=TRUE}
sigma1dev <- function (dat, sigma) {
  slag <- with(dat,sigma*Ps+zlag)
  fit <- glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(slag)),
             data=dat,subset=Ilag>0&I>0)
  fit$deviance
}

bake(file="sigma-profile.rds",{
  dat %>% subset(!(town%in%excludes)) -> dat1
  foreach (
    sigma=seq(0.03,0.25,length=100),
    .combine=rbind,.inorder=FALSE,
    .packages=c("plyr","magrittr")) %dopar% {
      dat1 %>%
        daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>%
        sum() -> dev
      data.frame(sigma=sigma,dev=dev)
    } -> sigmaProf
  
}) -> sigmaProf
```

```{r sigma-bar,purl=TRUE}
bake(file="sigma-bar.rds",{
  dat %>% subset(!(town%in%excludes)) -> dat1
  fit <- optim(par=0.037,lower=0.03,upper=0.10,
               method="Brent",hessian=TRUE,
               fn=function (sigma) {
                 dat1 %>%
                   daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>%
                   sum()
               })
  fit$par -> sigmaBar
}) -> sigmaBar

dat %>% mutate(Slag=sigmaBar*Ps+zlag) -> dat
```

```{r sigma-profile-plot,purl=FALSE}
sigmaProf %>% ggplot(aes(x=sigma,y=dev))+geom_line()
```

Our global sigma estimate is $\sigma=`r signif(sigmaBar,3)`$.
Now, we assume this value of $\sigma$ and fit TSIR to each of the towns individually using linear regression.

```{r fit-tsir,purl=TRUE}
bake(file="tsir-fits.rds",{
  coefnames <- c(sprintf("seas%d",1:26),"log(Ilag)","I(1/Ilag)")
  newcoefnames <- c(sprintf("log.beta%02d",1:26),"alpha","m.alpha")
  
  tsirfit <- function (dat) {
    glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(Slag)),
      data=dat,subset=Ilag>0&I>0) %>% summary() %>%
      extract2("coefficients") -> fit
    fit[,"Estimate"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(newcoefnames) -> coefs
    fit[,"Std. Error"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(paste(newcoefnames,"se",sep=".")) -> se
    cbind(coefs,se,sigma=sigmaBar,
      town=unique(dat$town),Ps=unique(dat$Ps))
  }
  
  dat %>% ddply(~town,tsirfit,.parallel=TRUE) %>%
    melt(id=c("town","Ps")) %>%
    mutate(se=ifelse(grepl("\\.se$",variable),"se","est"),
      variable=sub("\\.se$","",variable)) %>%
    dcast(town+Ps+variable~se) -> tsirs
  
  dat %>% ddply(~town,summarize,Ps=unique(Ps)) -> tsircoef
  
  tsirs %>%
    na.omit() %>%
    ddply(~variable, function (d) {
      fit <- lm(est~log(Ps),data=d,weights=1/se^2)
      data.frame(town=tsircoef$town,value=predict(fit,newdata=tsircoef))
    },.parallel=TRUE) %>%
    dcast(town~variable) %>%
    melt(id=c("town","alpha","m.alpha"),value.name="log.beta") %>%
    mutate(biweek=as.integer(sub("log.beta","",as.character(variable)))) %>%
    arrange(town,biweek) %>%
    subset(select=-variable) -> tsircoef
  
  dat %>%
    join(tsircoef,by=c("town","biweek")) %>%
    mutate(ylag=Ilag/Ps) -> dat
}) -> dat
```

Just for interest, let's plot $R_0$ as a function of city size.

```{r R0-plot,purl=FALSE}
dat %>% 
  ddply(~town,summarize,N=mean(Ps),
        beta=exp(mean(log.beta)),R0=beta*N) %>%
  ggplot(aes(x=N,y=R0))+geom_point()+
  scale_x_log10(breaks=10^seq(2,7),
                labels=trans_format('log10',math_format(10^.x)))+
  scale_y_log10(breaks=seq(1,30))+
  labs(x="Population size",y=expression(R[0]))+
  theme_classic()
```

- `Shat` is fitted to each individually.
- `Sbar` is fitted globally (with some towns excluded).

# Coupling the cities.

Let $X_{i}(t)$ be the observation at time $t$ in city $i$.
We assume that
$$X_{i}(t+1) \sim \dist{Poisson}{\lambda_i(t)}$$ 
or, alternatively,
$$X_{i}(t+1) \sim \dist{NegBinom}{\lambda_i(t),\frac{1}{\psi}},$$
where $\psi$ is an overdispersion parameter.
In the latter, we have parameterized the negative binomial distribution so that
$\expect{X_i(t+1)}=\lambda_i(t)$ and $\var{X_i(t+1)}=\lambda_i(t)+\psi\,\lambda_i(t)^2$.

When $I_i(t)=0$, we have that
$$\lambda_i(t) = \beta_i(t)\,S_i(t)\,\iota_i(t)^\alpha.$$
In the above, $\beta$ is constructed by fitting the TSIR model to each city independently and the susceptible pool, $S$, is reconstructed using TSIR methods.
The quantity $\iota$ is the import rate, which we estimate using a variety of different models.

The following computes $y$, $S$, $\beta$, and the matrix of reciprocal distances.
It also picks out the relevant observations.
These are the ones for which the preceding week saw zero cases.


```{r gravity-pre,purl=TRUE}
readRDS("tsir-fits.rds") -> dat
dat %>% acast(town~year+biweek,value.var="ylag") -> ylag
dat %>% acast(town~year+biweek,value.var="Slag") -> slag
dat %>% acast(town~year+biweek,value.var="log.beta") %>% exp() -> beta
dat %>% acast(town~year+biweek,value.var="cases") -> obs
dat %>% daply(~town,function(x)unique(x$Ps)) -> N

readRDS("distances.rds") -> distances
dd <- 1/distances
diag(dd) <- 0
dd <- dd[rownames(ylag),rownames(ylag)]

stopifnot(identical(rownames(ylag),rownames(slag)))
stopifnot(identical(rownames(ylag),rownames(beta)))
stopifnot(identical(rownames(ylag),rownames(obs)))
stopifnot(identical(rownames(ylag),names(N)))
stopifnot(identical(rownames(ylag),rownames(dd)))
stopifnot(identical(rownames(ylag),colnames(dd)))

ddscal <- exp(mean(log(dd[lower.tri(dd)])))
dd <- dd/ddscal

alpha <- mean(dat$alpha)

relevant <- which(ylag==0&slag>=0)
obs <- obs[relevant]
betaS <- beta[relevant]*slag[relevant]
```

```{r startMPI,cache=FALSE,include=FALSE,purl=TRUE}
if (file.exists("CLUSTER")) {
  scan("CLUSTER",what=integer(0)) -> ncpu
  library(doMPI)
  cl <- startMPIcluster(ncpu,verbose=TRUE,logdir="/tmp")
  registerDoMPI(cl)
} else {
  library(doMC)
  registerDoMC()
}
```

# The gravity model

The gravity model (with intercept) is
$$\iota_i=N_i^{\tau_1} \left(\theta\,\sum_{j\ne i}\! N_j^{\tau_2} d_{ij}^{-\rho}\,\frac{I_j}{N_j}+\phi\right).$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the gravity model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y+\phi\,N^{\tau_1}.$$

We use **R**'s element-recycling feature, which allows us to write
$$\mathrm{diag}(A)\,B=A\;*\;B.$$


The negative log likelihood function for the gravity model is:
```{r gravity-like,purl=TRUE}
likfnGravity <- function (theta, phi, rho, psi, tau1, tau2) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Now we'll do a likelihood profile over $\tau_1$ and $\tau_2$.

```{r gravity-profile-2D,cache=FALSE,purl=TRUE}
bake(file="gravity.rds",{
  
  grd <- profileDesign(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    lower=c(psi=log(5),theta=log(1e-8),phi=log(1e-6),rho=log(0.9)),
    upper=c(psi=log(7),theta=log(1),phi=log(1e-2),rho=log(1.7)),
    nprof=10
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnGravity) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnGravity,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~tau1+tau2,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnGravity) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnGravity,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  
  attr(results,"nproc") <- getDoParWorkers()
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster.
Now we refine to obtain the global MLE.

```{r gravity-mle,cache=FALSE}
bake("gravity-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnGravity,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.grav
```

```{r gravity-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(rho=exp(rho),theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>% count(~conv)

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.grav)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.grav,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.grav,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

Regarding NLOPT return values:

**Successful termination (positive return values)**

- NLOPT_SUCCESS = 1
Generic success return value.
- NLOPT_STOPVAL_REACHED = 2
Optimization stopped because stopval (above) was reached.
- NLOPT_FTOL_REACHED = 3
Optimization stopped because ftol_rel or ftol_abs (above) was reached.
- NLOPT_XTOL_REACHED = 4
Optimization stopped because xtol_rel or xtol_abs (above) was reached.
- NLOPT_MAXEVAL_REACHED = 5
Optimization stopped because maxeval (above) was reached.
- NLOPT_MAXTIME_REACHED = 6
Optimization stopped because maxtime (above) was reached.

**Error codes (negative return values)**

- NLOPT_FAILURE = -1
Generic failure code.
- NLOPT_INVALID_ARGS = -2
Invalid arguments (e.g. lower bounds are bigger than upper bounds, an unknown algorithm was specified, etcetera).
- NLOPT_OUT_OF_MEMORY = -3
Ran out of memory.
- NLOPT_ROUNDOFF_LIMITED = -4
Halted because roundoff errors limited progress. (In this case, the optimization still typically returns a useful result.)
- NLOPT_FORCED_STOP = -5
Halted because of a forced termination: the user called nlopt_force_stop(opt) on the optimization’s  nlopt_opt object opt from the user’s objective function or constraints.

```{r gravity-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+rho+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik)-10000,cex=0.5)
```

We look for evidence of overdispersion by computing the scale parameter for the negative binomial model.


# The Xia (wonky gravity) model

The model of @xia2004measles is
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_{j\ne i}\! I_j^{\tau_2} d_{ij}^{-\rho} + \phi\right).$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the @xia2004measles model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y^{\tau_2}+\phi\,N^{\tau_1}.$$

The negative log likelihood function for the @xia2004measles model is:

```{r xia-like,purl=TRUE}
likfnXia <- function (theta, phi, rho, psi, tau1, tau2) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag^tau2)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Again, a profile computation.

```{r xia-profile-2D,cache=FALSE,purl=TRUE}
bake(file="xia.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.5, length=25),
    tau2=seq(0.1, 1.0, length=25),
    psi=log(5),
    rho=log(1.0),
    theta=log(1e-6),
    phi=log(1e-6)
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnXia,
        method="Nelder-Mead",
        start=as.list(start[c("rho","theta","psi","phi")]),
        fixed=as.list(start[c("tau1","tau2")]),
        control=list(trace=0,maxit=10000),
        skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),
        conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r xia-mle,cache=FALSE}
bake("xia-mle.rds",{
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnXia,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.xia
```

```{r xia-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(rho=exp(rho),theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>% count(~conv)

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.xia)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.xia,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.xia,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r xia-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+rho+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik)-10000,cex=0.5)
```

# The mean field model

By setting $\rho = 0$ and $\tau_1=\tau_2=1$ in the gravity model, we obtain mean-field model.


The negative log likelihood function for the mean-field model is:
```{r meanfield-like,purl=TRUE}
likfnMeanField <- function (theta, phi, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  Q <- N*dd
  iota <- N*(theta*crossprod(Q,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Now we find the MLE.

```{r meanfield-optim,cache=FALSE,purl=TRUE}
bake(file="meanfield.rds",{

  grd <- sobolDesign(
    lower=c(psi=log(5),theta=log(1e-8),phi=log(1e-6)),
    upper=c(psi=log(7),theta=log(1),phi=log(1e-2)),
    nseq=250
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c()
      formals(likfnMeanField) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnMeanField,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c()
      formals(likfnMeanField) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnMeanField,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  results
  
}) -> results
```

```{r meanfield-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+psi,
  data=results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply(),
  subset=loglik>max(loglik)-10000,cex=0.5)
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster.
Now we refine to obtain the global MLE.

```{r meanfield-mle,cache=FALSE}
bake("meanfield-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnMeanField,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.meanfield
```

```{r meanfield-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>% count(~conv)
```


# Diffusion model

By setting $\tau_1 = \tau_2 = 0$ in the gravity model, we obtain a diffusion model, which couples cities in a way that depends only on the distance between them.

The negative log likelihood function for the diffusion model is:
```{r diffusion-like,purl=TRUE}
likfnDiffusion <- function (theta, phi, rho, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- dd^rho
  iota <- (theta*crossprod(Q,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Now we'll do a likelihood profile over $\tau_1$ and $\tau_2$.

```{r diffusion-profile-2D,cache=FALSE,purl=TRUE}
bake(file="diffusion.rds",{

  grd <- profileDesign(
    rho = seq(log(1),log(3),length=50),
    lower=c(psi=log(5),theta=log(1e-8),phi=log(1e-6)),
    upper=c(psi=log(7),theta=log(1),phi=log(1e-2)),
    nprof=10
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("rho")
      formals(likfnDiffusion) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnDiffusion,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~rho,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("rho")
      formals(likfnDiffusion) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnDiffusion,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  results
  
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster.
Now we refine to obtain the global MLE.

```{r diffusion-mle,cache=FALSE}
bake("diffusion-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnDiffusion,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.diffusion
```

```{r diffusion-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(theta=exp(theta),psi=exp(psi),phi=exp(phi),rho=exp(rho))

results %>% count(~conv)

results %>%
  ggplot(aes(x=rho,y=loglik))+
  geom_point()+
  labs(x=expression(rho),y=expression(log(L)))
```

```{r diffusion-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+psi+rho,
  data=results,
  subset=loglik>max(loglik)-10000,cex=0.5)
```


# Competing destinations model

The competing destinations model is
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_j\frac{N_j^{\tau_2}}{d_{ij}^\rho}\,\left(\sum_{k \ne i, j}\frac{N_k^{\tau_2}}{d_{jk}^\rho}\right)^\delta\,\frac{I_j}{N_j}+\phi\right).$$

Let
$$Q_{ij}=\begin{cases}{N_i^{\tau_2}}{d_{ij}^{-\rho}}, &i\ne j\\0, &i=j\end{cases}$$ and
$$R_{ji}=\sum_{k \ne i, j}{N_k^{\tau_2}}{d_{jk}^{-\rho}}=\sum_{k\ne i,j}Q_{kj}=\sum_{k\ne i}Q_{kj}=\sum_{k}Q_{kj}-Q_{ij}.$$
This implies
$$R^T=(\mathbb{1}\,\mathbb{1}^T-I)\,Q \qquad \Longleftrightarrow \qquad R=Q^T\,(\mathbb{1}\,\mathbb{1}^T-I)$$
and
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_j Q_{ji}\,R_{ji}^\delta\,y_{j}+\phi\right).$$

The negative log likelihood function for the competing destinations model is:

```{r compdest-like,purl=TRUE}
iii <- 1-diag(length(N))

likfnCompDest <- function (theta, phi, rho, psi, tau1, tau2, delta) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  iota <- (N^tau1)*(theta*crossprod(Q*R,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

We compute the profile likelihood as before.

```{r compdest-profile-2D,cache=FALSE,purl=TRUE}
bake(file="compdest.rds",{
  
  grd <- profileDesign(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    lower=c(psi=log(5),theta=log(0.1),phi=log(1e-6),rho=log(1.5),delta=-3),
    upper=c(psi=log(7),theta=log(45),phi=log(1e-2),rho=log(30),delta=0),
    nprof=20
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~tau1+tau2,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r compdest-mle,cache=FALSE}
bake("compdest-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnCompDest,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.compdest
```


```{r compdest-results,purl=TRUE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(rho=exp(rho),theta=exp(theta),psi=exp(psi),phi=exp(phi)) -> results

results %>% count(~conv)
```

```{r compdest-plot,purl=FALSE}
results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.compdest)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.compdest,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.compdest,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r compdest-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+rho+delta+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik)-10000,cex=0.5)
```

## One-D profiles

### $\delta$

```{r compdest-profile-delta,cache=FALSE,purl=TRUE}
bake(file="compdest_delta.rds",{
  
  grd <- profileDesign(
    delta=seq(-1.6,0,length=250),
    lower=c(tau1=0.1,tau2=-1,psi=log(5),theta=log(0.1),phi=log(1e-6),rho=log(1.5)),
    upper=c(tau1=1.4,tau2=1,psi=log(7),theta=log(45),phi=log(1e-2),rho=log(30)),
    nprof=50
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("delta")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> res
  
  res %>%
    extract(sapply(res,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  res %>%
    extract(!sapply(res,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~delta,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("delta")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> res1
  
  attr(res1,"nproc") <- getDoParWorkers()
  
  res1
}) -> res1
```

```{r compdest-profile-delta-plot,purl=FALSE}
res1 %>%
  ldply() %>%
  ggplot(aes(x=delta,y=loglik))+
  geom_point()
```

The above was completed in `r round(attr(res1,"system.time")[3]/60,1)`&nbsp;min on a `r attr(res1,"nproc")`-core cluster. 

### $\tau_1$

```{r compdest-profile-tau1,cache=FALSE,purl=TRUE}
bake(file="compdest_tau1.rds",{
  
  grd <- profileDesign(
    tau1=seq(0.1,1.4,length=250),
    lower=c(tau2=-1,psi=log(5),theta=log(0.1),phi=log(1e-6),rho=log(1.5),delta=-3),
    upper=c(tau2=1,psi=log(7),theta=log(45),phi=log(1e-2),rho=log(30),delta=0),
    nprof=50
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> res
  
  res %>%
    extract(sapply(res,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  res %>%
    extract(!sapply(res,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~tau1,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> res2
  
  attr(res2,"nproc") <- getDoParWorkers()
  
  res2
}) -> res2
```

```{r compdest-profile-tau1-plot,purl=FALSE}
res2 %>%
  ldply() %>%
  ggplot(aes(x=tau1,y=loglik))+
  geom_point()
```

The above was completed in `r round(attr(res2,"system.time")[3]/60,1)`&nbsp;min on a `r attr(res2,"nproc")`-core cluster. 

### $\tau_2$

```{r compdest-profile-tau2,cache=FALSE,purl=TRUE}
bake(file="compdest_tau2.rds",{
  
  grd <- profileDesign(
    tau2=seq(-1,1,length=250),
    lower=c(tau1=0.1,psi=log(5),theta=log(0.1),phi=log(1e-6),rho=log(1.5),delta=-3),
    upper=c(tau1=1.4,psi=log(7),theta=log(45),phi=log(1e-2),rho=log(30),delta=0),
    nprof=50
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau2")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> res
  
  res %>%
    extract(sapply(res,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  res %>%
    extract(!sapply(res,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~tau2,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau2")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> res3
  
  attr(res3,"nproc") <- getDoParWorkers()
  
  res3
}) -> res3
```

```{r compdest-profile-tau2-plot,purl=FALSE}
res3 %>%
  ldply() %>%
  ggplot(aes(x=tau2,y=loglik))+
  geom_point()
```

The above was completed in `r round(attr(res3,"system.time")[3]/60,1)`&nbsp;min on a `r attr(res3,"nproc")`-core cluster. 


# Stouffer model

Let $i$ be the recipient town; $j$, the donor.
Let $S(i,j)$ be the collection of towns closer to town $i$ than $j$ is.
That is, $S(i,j) = \{k: k\ne i \ \&\ d(i,k) \le d(i,j)\}$.

$$\iota_i = N_i^{\tau_1}\,\left(\theta\,\sum_j\!\left(\frac{N_j}{\sum_{k\in S(i,j)}{N_k}}\right)^{\tau_2}\,\frac{I_j}{N_j}+\phi\right).$$



```{r rankmat,cache=FALSE,purl=TRUE}
bake(file="rankmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/(sum(N[distances[i,]<=distances[i,j]])-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

The negative log likelihood function for the @xia2004measles model is:

```{r stouffer-like,purl=TRUE}
likfnStouffer <- function (theta, phi, tau1, tau2, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*((rr^tau2)%*%ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

We compute the profile likelihood as before.

```{r stouffer-profile-2D,cache=FALSE,purl=TRUE}
bake(file="stouffer.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.5, 1.2, length=25),
    tau2=seq(0.5, 2.0, length=25),
    theta=log(0.2),
    phi=log(0.0001),
    psi=log(5)
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnStouffer,
        method="Nelder-Mead",
        start=as.list(start[c("theta","phi","psi")]),
        fixed=as.list(start[c("tau1","tau2")]),
        control=list(trace=0,maxit=10000),
        skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r stouffer-mle,cache=FALSE}
bake("stouffer-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnStouffer,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.stouffer
```

```{r stouffer-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  subset(is.finite(loglik)) %>%
  mutate(theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>% count(~conv)

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.stouffer)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.stouffer,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.stouffer,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r stouffer-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik,na.rm=TRUE)-10000,cex=0.5)
```

## Variant: Stouffer model with recipient included

Let $i$ be the recipient town; $j$, the donor.
Let $S'(i,j)$ be the collection of towns closer to town $i$ than $j$ is.
That is, $S'(i,j) = \{k: d(i,k) \le d(i,j)\}$.
Note that $S'(i,j)$ includes $i$, whilst $S(i,j)$ does not. 

$$\iota_i = N_i^{\tau_1}\,\left(\theta\,\sum_j\!\left(\frac{N_j}{\sum_{k\in S'(i,j)}{N_k}}\right)^{\tau_2}\,\frac{I_j}{N_j}+\phi\right)$$


```{r rankmat1,cache=FALSE,purl=TRUE}
bake(file="rankmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/sum(N[distances[i,]<=distances[i,j]])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r stouffer1-like,purl=TRUE}
likfnStouffer1 <- function (theta, phi, tau1, tau2, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*(rr^tau2)%*%ylag+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

The profile likelihood computation.

```{r stouffer1-profile-2D,cache=FALSE,purl=TRUE}
bake(file="stouffer1.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.5, 1.2, length=25),
    tau2=seq(0.5, 2.0, length=25),
    theta=log(0.2),
    phi=log(0.0001),
    psi=log(5)
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnStouffer1,
        method="Nelder-Mead",
        start=as.list(start[c("theta","phi","psi")]),
        fixed=as.list(start[c("tau1","tau2")]),
        control=list(trace=0,maxit=10000),
        skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r stouffer1-mle,cache=FALSE}
bake("stouffer1-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnStouffer1,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.stouffer1
```

```{r stouffer1-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  subset(is.finite(loglik))

results %>% count(~conv)

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.stouffer1)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.stouffer1,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.stouffer1,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r stouffer1-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik,na.rm=TRUE)-10000,cex=0.5)
```


# Radiation model

$$\iota_i=\theta \sum_j N_j \frac{N_j N_i}{(N_j + \sum_{k \in S(i,j)} N_k)(N_j + N_i + \sum_{k \in S(i,j)} N_k)} \frac{I_j}{N_j}$$

```{r radmat,cache=FALSE,purl=TRUE}
bake(file="radmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/s/(s-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r radiation-like}
likfnRadiation <- function (theta, psi) {
  theta <- exp(theta)
  iota <- theta*(rr%*%ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

```{r radiation-mle,cache=FALSE}
bake(file="radiation-mle.rds",{
  mle2(likfnRadiation,
    method="Nelder-Mead",
    start=list(theta=log(1),psi=log(5)),
    control=list(trace=0,maxit=10000),
    skip.hessian=FALSE)
}) -> fit
```

```{r radiation-results,purl=FALSE}
summary(fit)
logLik(fit)
c(coef(fit),loglik=logLik(fit)) %>%
  as.list() %>% as.data.frame() -> mle.radiation
```

## Variant: radiation model with recipient included

$$\iota_i=\theta \sum_j N_j \frac{N_j N_i}{(N_j + \sum_{k \in S'(i,j)} N_k)(N_j + N_i + \sum_{k \in S'(i,j)} N_k)} \frac{I_j}{N_j}$$

```{r radmat1,cache=FALSE,purl=TRUE}
bake(file="radmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/(s+N[i])/(s)
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r radiation1-like}
likfnRadiation1 <- function (theta, psi) {
  theta <- exp(theta)
  iota <- theta*(rr%*%ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

```{r radiation1-mle,cache=FALSE}
bake(file="radiation1-mle.rds",{
  mle2(likfnRadiation1,
    method="Nelder-Mead",
    start=list(theta=log(1),psi=log(5)),
    control=list(trace=0,maxit=10000),
    skip.hessian=FALSE)
}) -> fit
```

```{r radiation1-results,purl=FALSE}
summary(fit)
logLik(fit)
c(coef(fit),loglik=logLik(fit)) %>%
  as.list() %>% as.data.frame() -> mle.radiation1
```


# Model comparison

```{r modelcomp,cache=FALSE,echo=FALSE,purl=FALSE}
summaryGravity <- function (mle) {
  npar <- 6
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    rho <- exp(rho)
    Q <- (N^tau2)*(dd^rho)
    iota <- (N^tau1)*(theta*crossprod(Q,ylag)+phi)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>%
    cbind(mle)
}

summaryXia <- function (mle) {
  npar <- 6
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    rho <- exp(rho)
    Q <- (N^tau2)*(dd^rho)
    iota <- (N^tau1)*(theta*crossprod(Q,ylag^tau2)+phi)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>% 
    cbind(mle)
}

summaryMeanField <- function (mle) {
  npar <- 5
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    Q <- N*dd
    iota <- N*(theta*crossprod(Q,ylag)+phi)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>%
    cbind(mle)
}

summaryDiffusion <- function (mle) {
  npar <- 4
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    rho <- exp(rho)
    Q <- (dd^rho)
    iota <- (theta*crossprod(Q,ylag)+phi)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>%
    cbind(mle)
}

summaryCompDest <- function (mle) {
  npar <- 7
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    rho <- exp(rho)
    Q <- (N^tau2)*(dd^rho)
    R <- crossprod(Q,iii)^delta
    iota <- (N^tau1)*(theta*crossprod(Q*R,ylag)+phi)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>% 
    cbind(mle)
}

summaryStouffer <- function (mle) {
  npar <- 5
  readRDS("rankmat.rds") -> rr
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    iota <- (N^tau1)*(theta*((rr^tau2)%*%ylag)+phi)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>% 
    cbind(mle)
}

summaryStouffer1 <- function (mle) {
  npar <- 5
  readRDS("rankmat1.rds") -> rr
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    iota <- (N^tau1)*(theta*(rr^tau2)%*%ylag+phi)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>% 
    cbind(mle)
}

summaryRadiation <- function (mle) {
  npar <- 2
  readRDS("radmat.rds") -> rr
  mle %$% {
    theta <- exp(theta)
    iota <- theta*(rr%*%ylag)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>% 
    cbind(mle)
}

summaryRadiation1 <- function (mle) {
  npar <- 2
  readRDS("radmat1.rds") -> rr
  mle %$% {
    theta <- exp(theta)
    iota <- theta*(rr%*%ylag)
    lambda <- betaS*iota[relevant]^alpha
    var <- lambda+lambda*lambda*exp(psi)
    data.frame(npar=npar,cHat=sum((obs-lambda)^2/var)/(length(obs)-npar))
  } %>% 
    cbind(mle)
}

mle.grav %>% summaryGravity() -> mle.grav
mle.xia %>% summaryXia() -> mle.xia
mle.meanfield %>% summaryMeanField() -> mle.meanfield
mle.diffusion %>% summaryDiffusion() -> mle.diffusion
mle.compdest %>% summaryCompDest() -> mle.compdest
mle.stouffer %>% summaryStouffer() -> mle.stouffer
mle.stouffer1 %>% summaryStouffer1() -> mle.stouffer1
mle.radiation %>% summaryRadiation() -> mle.radiation
mle.radiation1 %>% summaryRadiation1() -> mle.radiation1

dplyr::bind_rows(
  gravity=mle.grav,
  xia=mle.xia,
  meanfield=mle.meanfield,
  diffusion=mle.diffusion,
  comp.dest=mle.compdest,
  stouffer=mle.stouffer,
  stouffer.variant=mle.stouffer1,
  radiation=mle.radiation,
  radiation.variant=mle.radiation1,
  .id="model"
) %>%
  arrange(-loglik) %>%
  mutate(rho=exp(rho),psi=exp(psi),qaic=-2*loglik/cHat[1]+2*npar,delta.qaic=qaic-min(qaic,na.rm=TRUE)) %>%
  mutate(theta=theta/log(10),phi=phi/log(10)) %>%
  subset(select=c(model,loglik,qaic,cHat,delta.qaic,theta,phi,rho,tau1,tau2,delta,psi)) %>%
  rename(c(theta="$\\log_{10}{\\theta}$",cHat="$\\hat{c}$",phi="$\\log_{10}{\\phi}$",rho="$\\rho$",
    psi="$\\psi$",tau1="$\\tau_1$",tau2="$\\tau_2$",delta="$\\delta$",
    loglik="$\\ell$",qaic="$\\mathrm{QAIC}$",delta.qaic="$\\Delta\\!\\mathrm{QAIC}$")) %>%
  kable(digits=3)
```

# Diagnostics

## Import and export rates by population size

We can predict importation rates from fitted models

```{r matrices,purl=FALSE}
#Gravity
readRDS("gravity-mle.rds") %$% {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  (N^tau1)*(theta*t(Q))
} -> GRmat

#Xia
readRDS("xia-mle.rds") %$% {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  (N^tau1)*(theta*t(Q))
} -> XImat

#CD
readRDS("compdest-mle.rds") %$% {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  (N^tau1)*(theta*t(Q*R))
} -> CDmat

#Stouffer
readRDS("stouffer-mle.rds") %$% {
  readRDS("rankmat.rds") -> rr
  theta <- exp(theta)
  phi <- exp(phi)
  (N^tau1)*(theta*((rr^tau2)))
} -> STmat

#Stouffer variant
readRDS("stouffer1-mle.rds") %$% {
  readRDS("rankmat1.rds") -> rr
  theta <- exp(theta)
  phi <- exp(phi)
  (N^tau1)*(theta*((rr^tau2)))
} -> SVmat

#Radiation
mle.radiation %$% {
  readRDS("radmat.rds") -> rr
  theta <- exp(theta)
  theta*rr
} -> RDmat

#Radiation Variant
mle.radiation1 %$% {
  readRDS("radmat1.rds") -> rr
  theta <- exp(theta)
  theta*rr
} -> RVmat
```

```{r import_export,purl=FALSE}
data.frame(
  SV_import=SVmat %>% rowMeans(),
  ST_import=STmat %>% rowMeans(),
  GR_import=GRmat %>% rowMeans(),
  CD_import=CDmat %>% rowMeans(),
  RV_import=RVmat %>% rowMeans(),
  RD_import=RDmat %>% rowMeans(),
  SV_export=SVmat %>% colMeans(),
  ST_export=STmat %>% colMeans(),
  GR_export=GRmat %>% colMeans(),
  CD_export=CDmat %>% colMeans(),
  RV_export=RVmat %>% colMeans(),
  RD_export=RDmat %>% colMeans(),
  N=N
) %>% 
  name_rows() %>%
  dplyr::rename(town=.rownames) %>%
  tidyr::gather(variable,value,-N,-town) %>%
  tidyr::separate(variable,into=c("model","variable")) %>%
  tidyr::spread(variable,value) %>%
  dplyr::arrange(-N,town,model) %>%
  dplyr::mutate(erate=export/N,irate=import/N) %>%
  join(coords,by="town") -> impexp
```

```{r imp_exp_by_size,fig.height=6.83,purl=FALSE}
impexp %>%
  dplyr::select(town,N,model,export=erate,import=irate) %>%
  dplyr::filter(model %in% c("SV","CD","GR","RV")) %>%
  dplyr::mutate(
           model=factor(model,levels=c("SV","CD","GR","RV"))
         ) %>%
  tidyr::gather(rate,value,import,export) %>%
  ggplot(aes(x=N,y=value))+
  geom_point(alpha=0.3,size=0.1)+
  scale_x_log10()+
  scale_y_log10()+
  facet_grid(model~rate)
```

## Overall import and export rates

```{r imp_exp_maps,message=FALSE,results="hide",purl=FALSE}
readRDS("BritishIsles.rds") -> gb
gb %>%
  subset(NAME_1 %in% c("England","Wales")) %>%
  bbox() -> bbox

```
  
```{r ratemaps,message=FALSE,results="hide",purl=FALSE}
impexp %>%
  dplyr::select(town,model,export=erate,import=irate) %>%
  dplyr::filter(model %in% c("RV","GR","CD","SV")) %>%
  tidyr::gather(variable,value,export,import) %>%
  acast(model~town~variable,value.var="value") %>%
  aaply(c(2,3),function(x)outer(x,x,FUN="-")) %>%
  melt() %>%
  dplyr::rename(town=X1,variable=X2,m1=Var3,m2=Var4) %>% 
  mutate(
    m1=ordered(m1,levels=c("RV","GR","CD","SV")),
    m2=ordered(m2,levels=c("RV","GR","CD","SV"))
  ) %>%
  dplyr::filter(m1 > m2) %>% 
  dplyr::left_join(coords,by="town") -> ied

expand.grid(m2=1:4,m1=1:4) %>%
  dplyr::filter(m1<m2) %>%
  dplyr::mutate(
           model1=c("SV","CD","GR","RV")[m1],
           model2=c("SV","CD","GR","RV")[m2]
         ) -> comps
           
foreach (c=iter(comps,"row")) %do% {
  ied %>%
    dplyr::filter(m1==c$model1 & m2==c$model2) %>%
    arrange(variable,abs(value)) -> d
  gb %>%
    ggplot(aes(x=long,y=lat))+
    geom_map(aes(map_id=id),map=fortify(gb),fill=NA,color="black",size=0.3)+
    coord_map(projection="gilbert")+
    lims(x=bbox["x",],y=bbox["y",])+
    geom_point(
      data=d,
      aes(x=long,y=lat,color=value),
      size=1
    )+
    guides(alpha=FALSE)+
    scale_color_gradient2(mid="gray90")+
    labs(title="differences in predicted per capita import and export rates",
      subtitle=sprintf("%s - %s",c$model1,c$model2),
      color="",x="",y="")+
    facet_wrap(~variable,nrow=1)+
    theme(axis.text=element_blank()) -> pl
} -> ratemaps

print(ratemaps)
```

## Likelihood differences

```{r loglik_matrices,purl=FALSE,results="hide"}
dat %>% acast(town~year+biweek,value.var="cases") -> obs1

likGravity <- function (mle) {
  npar <- 6
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    rho <- exp(rho)
    Q <- (N^tau2)*(dd^rho)
    iota <- (N^tau1)*(theta*crossprod(Q,ylag)+phi)
    lambda <- betaS*iota[relevant]^alpha
    ll <- dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE)
    rv <- array(0,dim=dim(iota),dimnames=dimnames(iota))
    rv[relevant] <- ll
    rowSums(rv)/rowSums(rv!=0)
  }
}

likCompDest <- function (mle) {
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    rho <- exp(rho)
    Q <- (N^tau2)*(dd^rho)
    R <- crossprod(Q,iii)^delta
    iota <- (N^tau1)*(theta*crossprod(Q*R,ylag)+phi)
    lambda <- betaS*iota[relevant]^alpha
    ll <- dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE)
    rv <- array(0,dim=dim(iota),dimnames=dimnames(iota))
    rv[relevant] <- ll
    rowSums(rv)/rowSums(rv!=0)
  }
}

likStouffer1 <- function (mle) {
  readRDS("rankmat1.rds") -> rr
  mle %$% {
    theta <- exp(theta)
    phi <- exp(phi)
    iota <- (N^tau1)*(theta*(rr^tau2)%*%ylag+phi)
    lambda <- betaS*iota[relevant]^alpha
    ll <- dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE)
    rv <- array(0,dim=dim(iota),dimnames=dimnames(iota))
    rv[relevant] <- ll
    rowSums(rv)/rowSums(rv!=0)
  }
}

likRadiation1 <- function (mle) {
  readRDS("radmat1.rds") -> rr
  mle %$% {
    theta <- exp(theta)
    iota <- theta*(rr%*%ylag)
    lambda <- betaS*iota[relevant]^alpha
    ll <- dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE)
    rv <- array(0,dim=dim(iota),dimnames=dimnames(iota))
    rv[relevant] <- ll
    rowSums(rv)/rowSums(rv!=0)
  }
}

poplim <- 1e5

data.frame(
  SV=likStouffer1(mle.stouffer1),
  CD=likCompDest(mle.compdest),
  GR=likGravity(mle.grav),
  RV=likRadiation1(mle.radiation1),
  N=N
) %>%
  name_rows() %>%
  dplyr::rename(town=.rownames) %>%
  dplyr::left_join(coords,by="town") %>%
  dplyr::filter(N<poplim) %>%
  tidyr::gather(model,value,-N,-town,-long,-lat) -> spatialLiks

expand.grid(m2=1:4,m1=1:4) %>%
  dplyr::filter(m1<m2) %>%
  dplyr::mutate(
           model1=c("SV","CD","GR","RV")[m1],
           model2=c("SV","CD","GR","RV")[m2]
         ) -> comps

```

```{r loglik_by_size,purl=FALSE,results="hide"}
foreach (c=iter(comps,"row")) %do% {
  
  spatialLiks %>% 
    dplyr::filter(model %in% c("SV","CD","GR","RV")) %>%
    dplyr::mutate(
      model=factor(model,levels=c("SV","CD","GR","RV"))
    ) %>%
    dplyr::filter(model==c$model1) %>%
    dplyr::select(model,town,N,value) -> d1
  spatialLiks %>% 
    dplyr::filter(model %in% c("SV","CD","GR","RV")) %>%
    dplyr::mutate(
      model=factor(model,levels=c("SV","CD","GR","RV"))
    ) %>%
    dplyr::filter(model==c$model2) %>%
    dplyr::select(model,town,N,value) -> d2
  dplyr::full_join(d1,d2,by=c("town","N"),
                   suffix=c("_1","_2")) %>%
    mutate(diff=value_1-value_2) -> dd
  
 dd %>%
   ggplot(aes(x=N,y=diff,color=diff>0,group=1))+
   geom_point()+
   scale_x_log10()+
   guides(color=FALSE)+
   geom_smooth(method="loess")+
   scale_color_manual(values=c(`TRUE`=muted("blue"),`FALSE`=muted("red")))+
   labs(title="mean per datum differences in log likelihood",
        subtitle=sprintf("%s - %s",c$model1,c$model2),
        x="",y="",color="") -> pl
 
} -> likpl

print(likpl)

```

```{r likmaps,results="hide",purl=FALSE}
foreach (c=iter(comps,"row")) %do% {
  
  spatialLiks %>% dplyr::filter(model==c$model1) -> d1
  spatialLiks %>% dplyr::filter(model==c$model2) -> d2
  dplyr::full_join(d1,d2,by=c("town","N","long","lat"),
    suffix=c("_1","_2")) %>%
    mutate(diff=value_1-value_2) -> dd
  
 gb %>%
    ggplot(aes(x=long,y=lat))+
    geom_map(aes(map_id=id),map=fortify(gb),fill=NA,color="black",size=0.3)+
    coord_map(projection="gilbert")+
    lims(x=bbox["x",],y=bbox["y",])+
    geom_point(
      data=dd,
      aes(x=long,y=lat,color=diff,alpha=abs(diff)/max(abs(diff))),
      size=1
    )+
   guides(alpha=FALSE)+
   scale_color_gradient2(mid="gray90")+
   labs(title="per datum differences in log likelihood",
     subtitle=sprintf("%s - %s",c$model1,c$model2),
     x="",y="",color="")+
   theme(axis.text=element_blank()) -> pl
 
} -> likmaps

print(likmaps)

```

```{r loglik_lisa,results="hide",purl=FALSE}
foreach (c=iter(comps[c(2,4),],"row")) %do% {
  
  spatialLiks %>% dplyr::filter(model==c$model1) -> d1
  spatialLiks %>% dplyr::filter(model==c$model2) -> d2
  dplyr::full_join(d1,d2,by=c("town","N","long","lat"),
    suffix=c("_1","_2")) %>%
    mutate(diff=value_1-value_2) -> dd
  
  with(dd,
    ncf::lisa(x=long,y=lat,z=diff,neigh=30,latlon=TRUE,quiet=TRUE)
  ) -> ddlisa
  
  dd %>%
    dplyr::mutate(
      corr=ddlisa$correlation,
      mean=ddlisa$mean,
      p=ddlisa$p
    ) %>%
    dplyr::filter(p<0.05) -> dd

 gb %>%
    ggplot(aes(x=long,y=lat))+
    geom_map(aes(map_id=id),map=fortify(gb),fill=NA,color="black",size=0.3)+
    coord_map(projection="gilbert")+
    lims(x=bbox["x",],y=bbox["y",])+
    geom_point(
      data=dd,
      aes(x=long,y=lat,color=diff,alpha=0.5),
      size=1
    )+
   guides(alpha=FALSE)+
   scale_color_gradient2(mid="gray90")+
   labs(title="significant per datum differences in log likelihood",
     subtitle=sprintf("%s - %s",c$model1,c$model2),
     x="",y="",color="")+
   theme(axis.text=element_blank()) -> pl
 
} -> lisamaps

print(lisamaps)

```

The above plots are limited to cities of less than $`r poplim`$ souls, since larger cities have few or no fadeouts.


# Next steps

1. Exponential distance kernels
1. qAICs and qBayes-weights in table 
1. Random effects.
1. Revisit $\sigma$ profile
1. Revisit TSIR fitting (include residuals?)
1. Include references for the various models.

```{r session-info,cache=FALSE}
sessionInfo()
```

# References
