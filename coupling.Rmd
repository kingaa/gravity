---
title: "Spatial coupling in measles via fade-out survival analysis"
csl: ecology.csl
params:
  prefix: "coupling"
output:
  html_document:
    code_folding: show
    df_print: kable
    highlight: tango
    dev: png
    number_sections: no
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
    toc_depth: 2
    dev: png
    df_print: kable
bibliography: gravity.bib
---

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta[1]{{\Delta}{#1}}
\newcommand\lik{\mathscr{L}}
\newcommand\loglik{\ell}

Produced with **R** version `r getRversion()`.

--------------------------

```{r knitr-opts,include=FALSE,cache=FALSE}
library(knitr)
opts_chunk$set(
  eval=TRUE,
  purl=FALSE,
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  cache.extra=rand_seed,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",params$prefix,"-"),
  cache.path=paste0("cache/",params$prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
)
```

# Preparations

```{r prelims,cache=FALSE,purl=TRUE}
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  knitr.kable.NA="",
  encoding="UTF-8",
  aakmisc.dbname="ewmeasles",
  aakmisc.remotehost="kinglab.eeb.lsa.umich.edu",
  aakmisc.user="gravity")

set.seed(594709947L)

library(grid)
library(plyr)
library(reshape2)
library(magrittr)
library(foreach)
library(iterators)
library(bbmle)
library(nloptr)
library(pomp)
library(ggplot2)
theme_set(theme_bw())
library(doMC)
registerDoMC()
```

## Preprocess the data.

Retrieve the data from the database.

```{r clean-data,eval=TRUE,purl=TRUE}
library(aakmisc)

startTunnel()

getQuery("select town,year,births,pop from demog where year>=1944 order by town,year") -> demog

getQuery("select town,date,cases from measles where year>=1944 order by town,date") %>%
  mutate(year=as.integer(format(date+3,"%Y"))) %>%
  ddply(~town+year,mutate,week=seq_along(year),biweek=(week+1)%/%2) %>%
  subset(week<=52,select=-c(date,week)) %>%
  acast(town~year~biweek,value.var="cases",fun.aggregate=sum) %>%
  melt(varnames=c("town","year","biweek"),value.name="cases") %>%
  mutate(town=as.character(town)) %>%
  arrange(town,year,biweek) %>%
  join(demog,by=c("town","year")) %>%
  mutate(births=births/26) -> dat

stopTunnel()
```

In the above:

1. Measles data is weekly; dates given correspond to Sunday.
1. Associate each week's data with the Weds. of that week.
1. Discard any 53rd weeks (1947,1952,1958,1964).
1. Aggregate by biweek (26 biweeks/yr).
1. Join with demographic data.
1. Scale birth rates to births/biweek.

Now let's compute the distances between the cities.

```{r distances,eval=FALSE,purl=TRUE}
bake(file="distances.rds",{
  library(aakmisc)
  
  startTunnel()
  getQuery("select * from coords") %>% arrange(town) -> coords
  stopTunnel()
  rownames(coords) <- coords$town
  
  library(geosphere)
  distm(coords %>% subset(select=c(long,lat)),
    coords %>% subset(select=c(long,lat))) %>%
    matrix(nrow=nrow(coords),ncol=nrow(coords),
      dimnames=list(coords$town,coords$town))
}) -> distances
```

## Apply TSIR model to the individual towns.

### Susceptible reconstruction.

Smoothing spline regression of cumulative cases on cumulative births to estimate under-reporting and residuals.
Correct for under-reporting: `I = cases/ur`.

Regress cumulative cases on cumulative births to estimate under-reporting (ur) and susceptible depletion (`z`) `I` is estimated as `cases/ur`.

```{r under-reporting,eval=FALSE,purl=TRUE}
foreach (d=dlply(dat,~town),.combine=rbind,.inorder=FALSE) %dopar% {
  cumbirths <- cumsum(d$births)
  cumcases <- cumsum(d$cases)
  fit <- smooth.spline(cumbirths,cumcases,df=2.5)
  mutate(d,
    ur=predict(fit,x=cumbirths,deriv=1)$y,
    I=cases/ur)
} -> dat
```

Now reconstruct the susceptibles (via the residuals `z`).

```{r susceptible-reconstruction,eval=FALSE,purl=TRUE}
foreach (d=dlply(dat,~town),.combine=rbind,.inorder=FALSE) %dopar% {
  cumbirths <- cumsum(d$births)
  cuminc <- cumsum(d$I)
  fit <- smooth.spline(cumbirths,cuminc,df=2.5)
  mutate(d,z=-residuals(fit))
} -> dat
```

*Does the preceding scale the z variable appropriately?*
*I.e., does it take under-reporting into account?*

### Fit TSIR

First, set up the data matrix for the regression.
This involves lagging the I and z variables.
Population size is assumed constant at its median value.

```{r make-data-matrix,eval=FALSE,purl=TRUE}
dat %>%
  ddply(~town,mutate,
    Ilag=c(NA,head(I,-1)),
    zlag=c(NA,head(z,-1)),
    Ps=median(pop),
    seas=factor(biweek,levels=1:26)) %>%
  ddply(~town,tail,-1) -> dat
```

We'll start by estimating the mean susceptible fraction, assuming a single global value for this parameter.
Some towns have very high changes in susceptible fraction; exclude these.

```{r exclude-weird-towns,eval=FALSE,purl=TRUE}
dat %>%
  ddply(~town,summarize,
    maxfrac=max(-z/Ps)) %>%
  subset(maxfrac>0.03) %>%
  extract2("town") -> excludes

print(length(excludes))
```

Now profile on the fraction, $\sigma$, of susceptibles in the population.
This assumes a single, global value of $\sigma$.
**NOTE: the $\beta$ are here scaled by population size.**

```{r sigma-profile,eval=FALSE,purl=TRUE}
dat %>% subset(!(town%in%excludes)) -> dat1

sigma1dev <- function (dat, sigma) {
  slag <- with(dat,sigma*Ps+zlag)
  fit <- glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(slag)),
    data=dat,subset=Ilag>0&I>0)
  fit$deviance
}

foreach (sigma=seq(0.03,0.25,length=100),
  .combine=rbind,.inorder=FALSE) %dopar%
  {
    dat1 %>%
      daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>%
      sum() -> dev
    data.frame(sigma=sigma,dev=dev)
  } -> sigmaProf

sigmaProf %>% saveRDS("sigma-profile.rds")

fit <- optim(par=0.037,lower=0.03,upper=0.10,
  method="Brent",hessian=TRUE,
  fn=function (sigma) {
    dat1 %>%
      daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>%
      sum()
  })
fit$par -> sigmaBar

rm(dat1)

sigmaBar %>% saveRDS("sigma-bar.rds")

dat %>% mutate(Slag=sigmaBar*Ps+zlag) -> dat
```

```{r sigma-profile-plot}
readRDS("sigma-bar.rds") -> sigmaBar
readRDS("sigma-profile.rds") %>%
  ggplot(aes(x=sigma,y=dev))+geom_line()
```

Our global sigma estimate is $\sigma=`r signif(sigmaBar,3)`$.
Now, we assume this value of $\sigma$ and fit TSIR to each of the towns individually using linear regression.

```{r fit-tsir,purl=TRUE}
bake(file="tsir-fits.rds",{
  coefnames <- c(sprintf("seas%d",1:26),"log(Ilag)","I(1/Ilag)")
  newcoefnames <- c(sprintf("log.beta%02d",1:26),"alpha","m.alpha")
  
  tsirfit <- function (dat) {
    glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(Slag)),
      data=dat,subset=Ilag>0&I>0) %>% summary() %>%
      extract2("coefficients") -> fit
    fit[,"Estimate"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(newcoefnames) -> coefs
    fit[,"Std. Error"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(paste(newcoefnames,"se",sep=".")) -> se
    cbind(coefs,se,sigma=sigmaBar,
      town=unique(dat$town),Ps=unique(dat$Ps))
  }
  
  dat %>% ddply(~town,tsirfit,.parallel=TRUE) %>%
    melt(id=c("town","Ps")) %>%
    mutate(se=ifelse(grepl("\\.se$",variable),"se","est"),
      variable=sub("\\.se$","",variable)) %>%
    dcast(town+Ps+variable~se) -> tsirs
  
  dat %>% ddply(~town,summarize,Ps=unique(Ps)) -> tsircoef
  
  tsirs %>%
    na.omit() %>%
    ddply(~variable, function (d) {
      fit <- lm(est~log(Ps),data=d,weights=1/se^2)
      data.frame(town=tsircoef$town,value=predict(fit,newdata=tsircoef))
    },.parallel=TRUE) %>%
    dcast(town~variable) %>%
    melt(id=c("town","alpha","m.alpha"),value.name="log.beta") %>%
    mutate(biweek=as.integer(sub("log.beta","",as.character(variable)))) %>%
    arrange(town,biweek) %>%
    subset(select=-variable) -> tsircoef
  
  dat %>%
    join(tsircoef,by=c("town","biweek")) %>%
    mutate(ylag=Ilag/Ps) -> dat
}) -> dat
```

Just for interest, let's plot $R_0$ as a function of city size.

```{r R0-plot}
library(scales)
dat %>% ddply(~town,summarize,N=mean(Ps),
  beta=exp(mean(log.beta)),R0=beta*N) %>%
  ggplot(aes(x=N,y=R0))+geom_point()+
  scale_x_log10(breaks=10^seq(2,7),
    labels=trans_format('log10',math_format(10^.x)))+
  scale_y_log10(breaks=seq(1,30))+
  labs(x="Population size",y=expression(R[0]))+
  theme_classic()
```

- `Shat` is fitted to each individually.
- `Sbar` is fitted globally (with some towns excluded).

# Coupling the cities.

Let $X_{i}(t)$ be the observation at time $t$ in city $i$.
We assume that
$$X_{i}(t+1) \sim \dist{Poisson}{\lambda_i(t)}$$ 
or, alternatively,
$$X_{i}(t+1) \sim \dist{NegBinom}{\lambda_i(t),\frac{1}{\psi}},$$
where $\psi$ is an overdispersion parameter.
In the latter, we have parameterized the negative binomial distribution so that
$\expect{X_i(t+1)}=\lambda_i(t)$ and $\var{X_i(t+1)}=\lambda_i(t)+\psi\,\lambda_i(t)^2$.

When $I_i(t)=0$, we have that
$$\lambda_i(t) = \beta_i(t)\,S_i(t)\,\iota_i(t)^\alpha.$$
In the above, $\beta$ is constructed by fitting the TSIR model to each city independently and the susceptible pool, $S$, is reconstructed using TSIR methods.
The quantity $\iota$ is the import rate, which we estimate using a variety of different models.

The following computes $y$, $S$, $\beta$, and the matrix of reciprocal distances.
It also picks out the relevant observations.
These are the ones for which the preceding week saw zero cases.


```{r gravity-pre,purl=TRUE}
readRDS("tsir-fits.rds") -> dat
dat %>% acast(town~year+biweek,value.var="ylag") -> ylag
dat %>% acast(town~year+biweek,value.var="Slag") -> slag
dat %>% acast(town~year+biweek,value.var="log.beta") %>% exp() -> beta
dat %>% acast(town~year+biweek,value.var="cases") -> obs
dat %>% daply(~town,function(x)unique(x$Ps)) -> N

readRDS("distances.rds") -> distances
dd <- 1/distances
diag(dd) <- 0

stopifnot(identical(rownames(ylag),rownames(slag)))
stopifnot(identical(rownames(ylag),rownames(beta)))
stopifnot(identical(rownames(ylag),rownames(obs)))
stopifnot(identical(rownames(ylag),names(N)))
stopifnot(identical(rownames(ylag),rownames(dd)))
stopifnot(identical(rownames(ylag),colnames(dd)))

ddscal <- exp(mean(log(dd[lower.tri(dd)])))
dd <- dd/ddscal

alpha <- mean(dat$alpha)

relevant <- which(ylag==0&slag>=0)
obs <- obs[relevant]
betaS <- beta[relevant]*slag[relevant]
```

# The gravity model

The gravity model (with intercept) is
$$\iota_i=N_i^{\tau_1} \left(\theta\,\sum_{j\ne i}\! N_j^{\tau_2} d_{ij}^{-\rho}\,\frac{I_j}{N_j}+\phi\right).$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the gravity model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y+\phi\,N^{\tau_1}.$$

We use **R**'s element-recycling feature, which allows us to write
$$\mathrm{diag}(A)\,B=A\;*\;B.$$


The negative log likelihood function for the gravity model is:
```{r gravity-like,purl=TRUE}
likfnGravity <- function (theta, phi, rho, psi, tau1, tau2) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Now we'll do a likelihood profile over $\tau_1$ and $\tau_2$.

```{r gravity-profile-2D,cache=FALSE,purl=TRUE}
bake(file="gravity.rds",{
  
  grd <- profileDesign(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    lower=c(psi=log(5),theta=log(1e-8),phi=log(1e-6),rho=log(0.9)),
    upper=c(psi=log(7),theta=log(1),phi=log(1e-2),rho=log(1.7)),
    nprof=10
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnGravity) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnGravity,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~tau1+tau2,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnGravity) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnGravity,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  
  attr(results,"nproc") <- getDoParWorkers()
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster.
Now we refine to obtain the global MLE.

```{r gravity-mle,cache=FALSE}
bake("gravity-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnGravity,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.grav
```

```{r gravity-results}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(rho=exp(rho),theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>% count(~conv)

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.grav)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.grav,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.grav,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

Regarding NLOPT return values:

**Successful termination (positive return values)**

- NLOPT_SUCCESS = 1
Generic success return value.
- NLOPT_STOPVAL_REACHED = 2
Optimization stopped because stopval (above) was reached.
- NLOPT_FTOL_REACHED = 3
Optimization stopped because ftol_rel or ftol_abs (above) was reached.
- NLOPT_XTOL_REACHED = 4
Optimization stopped because xtol_rel or xtol_abs (above) was reached.
- NLOPT_MAXEVAL_REACHED = 5
Optimization stopped because maxeval (above) was reached.
- NLOPT_MAXTIME_REACHED = 6
Optimization stopped because maxtime (above) was reached.

**Error codes (negative return values)**

- NLOPT_FAILURE = -1
Generic failure code.
- NLOPT_INVALID_ARGS = -2
Invalid arguments (e.g. lower bounds are bigger than upper bounds, an unknown algorithm was specified, etcetera).
- NLOPT_OUT_OF_MEMORY = -3
Ran out of memory.
- NLOPT_ROUNDOFF_LIMITED = -4
Halted because roundoff errors limited progress. (In this case, the optimization still typically returns a useful result.)
- NLOPT_FORCED_STOP = -5
Halted because of a forced termination: the user called nlopt_force_stop(opt) on the optimization’s  nlopt_opt object opt from the user’s objective function or constraints.

```{r gravity-pairsplot,fig.height=6.83}
pairs(~loglik+theta+phi+rho+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik)-10000,cex=0.5)
```

We look for evidence of overdispersion by computing the scale parameter for the negative binomial model.


# The Xia (wonky gravity) model

The model of @xia2004measles is
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_{j\ne i}\! I_j^{\tau_2} d_{ij}^{-\rho} + \phi\right).$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the @xia2004measles model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y^{\tau_2}+\phi\,N^{\tau_1}.$$

The negative log likelihood function for the @xia2004measles model is:

```{r xia-like,purl=TRUE}
likfnXia <- function (theta, phi, rho, psi, tau1, tau2) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag^tau2)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Again, a profile computation.

```{r xia-profile-2D,cache=FALSE,purl=TRUE}
bake(file="xia.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.5, length=25),
    tau2=seq(0.1, 1.0, length=25),
    psi=log(5),
    rho=log(1.0),
    theta=log(1e-6),
    phi=log(1e-6)
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnXia,
        method="Nelder-Mead",
        start=as.list(start[c("rho","theta","psi","phi")]),
        fixed=as.list(start[c("tau1","tau2")]),
        control=list(trace=0,maxit=10000),
        skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),
        conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r xia-mle,cache=FALSE}
bake("xia-mle.rds",{
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnXia,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.xia
```

```{r xia-results}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(rho=exp(rho),theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.xia)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.xia,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.xia,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r xia-pairsplot,fig.height=6.83}
pairs(~loglik+theta+phi+rho+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik)-10000,cex=0.5)
```

# Competing destinations model

The competing destinations model is
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_j\frac{N_j^{\tau_2}}{d_{ij}^\rho}\,\left(\sum_{k \ne i, j}\frac{N_k^{\tau_2}}{d_{jk}^\rho}\right)^\delta\,\frac{I_j}{N_j}+\phi\right).$$

Let
$$Q_{ij}=\begin{cases}{N_i^{\tau_2}}{d_{ij}^{-\rho}}, &i\ne j\\0, &i=j\end{cases}$$ and
$$R_{ji}=\sum_{k \ne i, j}{N_k^{\tau_2}}{d_{jk}^{-\rho}}=\sum_{k\ne i,j}Q_{kj}=\sum_{k\ne i}Q_{kj}=\sum_{k}Q_{kj}-Q_{ij}.$$
This implies
$$R^T=(\mathbb{1}\,\mathbb{1}^T-I)\,Q \qquad \Longleftrightarrow \qquad R=Q^T\,(\mathbb{1}\,\mathbb{1}^T-I)$$
and
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_j Q_{ji}\,R_{ji}^\delta\,y_{j}+\phi\right).$$

The negative log likelihood function for the competing destinations model is:

```{r compdest-like,purl=TRUE}
iii <- 1-diag(length(N))

likfnCompDest <- function (theta, phi, rho, psi, tau1, tau2, delta) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  iota <- (N^tau1)*(theta*crossprod(Q*R,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

We compute the profile likelihood as before.

```{r compdest-profile-2D,cache=FALSE,purl=TRUE}
bake(file="compdest.rds",{
  
  grd <- profileDesign(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    lower=c(psi=log(5),theta=log(0.1),phi=log(1e-6),rho=log(1.5),delta=-3),
    upper=c(psi=log(7),theta=log(45),phi=log(1e-2),rho=log(30),delta=0),
    nprof=20
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1000,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique() %>%
    print()
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    ddply(~tau1+tau2,subset,loglik==max(loglik,na.rm=TRUE)) -> grd
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages=c("nloptr","magrittr")
  ) %dopar% try(
    {
      fixed <- c("tau1","tau2")
      formals(likfnCompDest) %>% names() %>% setdiff(fixed) -> est
      nloptr(unlist(start[est]),
        function(x){
          do.call(likfnCompDest,
            c(start[fixed],setNames(as.list(x),est)))
        },
        opts=list(
          algorithm="NLOPT_LN_SBPLX",
          ftol_abs=1e-5,
          xtol_rel=1e-7,
          maxeval=10000)
      ) -> fit
      fit$solution %>% setNames(est) %>%
        c(unlist(start[fixed]),loglik=-fit$objective) %>%
        as.list() %>% as.data.frame() %>%
        cbind(conv=fit$status) -> res
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r compdest-mle,cache=FALSE}
bake("compdest-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnCompDest,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.compdest
```


```{r compdest-results}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  mutate(rho=exp(rho),theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.compdest)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.compdest,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.compdest,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r compdest-pairsplot,fig.height=6.83}
pairs(~loglik+theta+phi+rho+delta+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik)-10000,cex=0.5)
```

# Stouffer model

Let $i$ be the recipient town; $j$, the donor.
Let $S(i,j)$ be the collection of towns closer to town $i$ than $j$ is.
That is, $S(i,j) = \{k: k\ne i \ \&\ d(i,k) \le d(i,j)\}$.

$$\iota_i = N_i^{\tau_1}\,\left(\theta\,\sum_j\!\left(\frac{N_j}{\sum_{k\in S(i,j)}{N_k}}\right)^{\tau_2}\,\frac{I_j}{N_j}+\phi\right).$$


```{r rankmat,cache=FALSE,purl=TRUE}
bake(file="rankmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/(sum(N[distances[i,]<=distances[i,j]])-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

The negative log likelihood function for the @xia2004measles model is:

```{r stouffer-like,purl=TRUE}
likfnStouffer <- function (theta, phi, tau1, tau2, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*((rr^tau2)%*%ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

We compute the profile likelihood as before.

```{r stouffer-profile-2D,cache=FALSE,purl=TRUE}
bake(file="stouffer.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.5, 1.2, length=25),
    tau2=seq(0.5, 2.0, length=25),
    theta=log(0.2),
    phi=log(0.0001),
    psi=log(5)
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnStouffer,
        method="Nelder-Mead",
        start=as.list(start[c("theta","phi","psi")]),
        fixed=as.list(start[c("tau1","tau2")]),
        control=list(trace=0,maxit=10000),
        skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r stouffer-mle,cache=FALSE}
bake("stouffer-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnStouffer,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.stouffer
```

```{r stouffer-results}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  subset(is.finite(loglik)) %>%
  mutate(theta=exp(theta),psi=exp(psi),phi=exp(phi))

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.stouffer)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.stouffer,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.stouffer,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r stouffer-pairsplot,fig.height=6.83}
pairs(~loglik+theta+phi+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik,na.rm=TRUE)-10000,cex=0.5)
```

## Variant: Stouffer model with recipient included

Let $i$ be the recipient town; $j$, the donor.
Let $S'(i,j)$ be the collection of towns closer to town $i$ than $j$ is.
That is, $S'(i,j) = \{k: d(i,k) \le d(i,j)\}$.
Note that $S'(i,j)$ includes $i$, whilst $S(i,j)$ does not. 

$$\iota_i = N_i^{\tau_1}\,\left(\theta\,\sum_j\!\left(\frac{N_j}{\sum_{k\in S'(i,j)}{N_k}}\right)^{\tau_2}\,\frac{I_j}{N_j}+\phi\right)$$


```{r rankmat1,cache=FALSE,purl=TRUE}
bake(file="rankmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/sum(N[distances[i,]<=distances[i,j]])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r stouffer1-like,purl=TRUE}
likfnStouffer1 <- function (theta, phi, tau1, tau2, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*(rr^tau2)%*%ylag+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

The profile likelihood computation.

```{r stouffer1-profile-2D,cache=FALSE,purl=TRUE}
bake(file="stouffer1.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.5, 1.2, length=25),
    tau2=seq(0.5, 2.0, length=25),
    theta=log(0.2),
    phi=log(0.0001),
    psi=log(5)
  )
  
  foreach(start=iter(grd,"row"),
    .errorhandling="pass",
    .inorder=FALSE,
    .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnStouffer1,
        method="Nelder-Mead",
        start=as.list(start[c("theta","phi","psi")]),
        fixed=as.list(start[c("tau1","tau2")]),
        control=list(trace=0,maxit=10000),
        skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r stouffer1-mle,cache=FALSE}
bake("stouffer1-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnStouffer1,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.stouffer1
```

```{r stouffer1-results}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() %>%
  subset(is.finite(loglik))

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.stouffer1)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.stouffer1,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.stouffer1,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r stouffer1-pairsplot,fig.height=6.83}
pairs(~loglik+theta+phi+psi+tau1+tau2,
  data=results,
  subset=loglik>max(loglik,na.rm=TRUE)-10000,cex=0.5)
```


# Radiation model

$$\iota_i=\theta \sum_j N_j \frac{N_j N_i}{(N_j + \sum_{k \in S(i,j)} N_k)(N_j + N_i + \sum_{k \in S(i,j)} N_k)} \frac{I_j}{N_j}$$

```{r radmat,cache=FALSE,purl=TRUE}
bake(file="radmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/s/(s-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r radiation-like}
likfnRadiation <- function (theta, psi) {
  theta <- exp(theta)
  iota <- theta*(rr%*%ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

```{r radiation-mle,cache=FALSE}
bake(file="radiation-mle.rds",{
  mle2(likfnRadiation,
    method="Nelder-Mead",
    start=list(theta=log(1),psi=log(5)),
    control=list(trace=0,maxit=10000),
    skip.hessian=FALSE)
}) -> fit
```

```{r radiation-results}
summary(fit)
logLik(fit)
c(coef(fit),loglik=logLik(fit)) %>%
  as.list() %>% as.data.frame() -> mle.radiation
```

## Variant: radiation model with recipient included

$$\iota_i=\theta \sum_j N_j \frac{N_j N_i}{(N_j + \sum_{k \in S'(i,j)} N_k)(N_j + N_i + \sum_{k \in S'(i,j)} N_k)} \frac{I_j}{N_j}$$

```{r radmat1,cache=FALSE,purl=TRUE}
bake(file="radmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/(s+N[i])/(s)
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r radiation1-like}
likfnRadiation1 <- function (theta, psi) {
  theta <- exp(theta)
  iota <- theta*(rr%*%ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

```{r radiation1-mle,cache=FALSE}
bake(file="radiation1-mle.rds",{
  mle2(likfnRadiation1,
    method="Nelder-Mead",
    start=list(theta=log(1),psi=log(5)),
    control=list(trace=0,maxit=10000),
    skip.hessian=FALSE)
}) -> fit
```

```{r radiation1-results}
summary(fit)
logLik(fit)
c(coef(fit),loglik=logLik(fit)) %>%
  as.list() %>% as.data.frame() -> mle.radiation1
```


# Model comparison

```{r modelcomp,cache=FALSE,echo=FALSE}
scalGravity <- function (theta, phi, rho, psi, tau1, tau2, loglik) {
  npar <- 6
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  var <- lambda+lambda*lambda*exp(psi)
  sum((obs-lambda)^2/var)/(length(obs)-npar)
}

scalXia <- function (theta, phi, rho, psi, tau1, tau2, loglik) {
  npar <- 6
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag^tau2)+phi)
  lambda <- betaS*iota[relevant]^alpha
  var <- lambda+lambda*lambda*exp(psi)
  sum((obs-lambda)^2/var)/(length(obs)-npar)
}

scalCompDest <- function (theta, phi, rho, psi, tau1, tau2, delta, loglik) {
  npar <- 7
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  iota <- (N^tau1)*(theta*crossprod(Q*R,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  var <- lambda+lambda*lambda*exp(psi)
  sum((obs-lambda)^2/var)/(length(obs)-npar)
}

scalStouffer <- function (theta, phi, tau1, tau2, psi, loglik) {
  npar <- 5
  readRDS("rankmat.rds") -> rr
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*((rr^tau2)%*%ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  var <- lambda+lambda*lambda*exp(psi)
  sum((obs-lambda)^2/var)/(length(obs)-npar)
}

scalStouffer1 <- function (theta, phi, tau1, tau2, psi, loglik) {
  npar <- 5
  readRDS("rankmat1.rds") -> rr
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*(rr^tau2)%*%ylag+phi)
  lambda <- betaS*iota[relevant]^alpha
  var <- lambda+lambda*lambda*exp(psi)
  sum((obs-lambda)^2/var)/(length(obs)-npar)
}

mle.grav %<>% mutate(cHat=scalGravity(theta,phi,rho,psi,tau1,tau2,loglik))
mle.xia %<>% mutate(cHat=scalXia(theta,phi,rho,psi,tau1,tau2,loglik))
mle.compdest %<>% mutate(cHat=scalCompDest(theta,phi,rho,psi,tau1,tau2,delta,loglik))
mle.stouffer %<>% mutate(cHat=scalStouffer(theta,phi,tau1,tau2,psi,loglik))
mle.stouffer1 %<>% mutate(cHat=scalStouffer1(theta,phi,tau1,tau2,psi,loglik))

list(gravity=mle.grav,
  xia=mle.xia,
  comp.dest=mle.compdest,
  stouffer=mle.stouffer,
  stouffer.variant=mle.stouffer1,
  radiation=mle.radiation,
  radiation.variant=mle.radiation1) %>%
  ldply(.id="model") %>%
  arrange(-loglik) %>%
  mutate(rho=exp(rho),psi=exp(psi),delta.loglik=loglik-max(loglik)) %>%
  subset(select=c(model,loglik,cHat,delta.loglik,theta,phi,rho,tau1,tau2,delta,psi)) %>%
  rename(c(theta="$\\log{\\theta}$",cHat="$\\hat{c}$",phi="$\\log{\\phi}$",rho="$\\rho$",
    psi="$\\psi$",tau1="$\\tau_1$",tau2="$\\tau_2$",delta="$\\delta$",
    loglik="$\\ell$",delta.loglik="$\\Delta\\!\\ell$")) %>%
  kable(digits=3)
```

# Diagnostics

To study whether better statistical fit translates to meaningful improvement on prediction on incidence we can look at CCS patterns:

```
dat$E=dat$cases==0 
dat %>% ddply(~town,summarize,mean(E)) -> ext
plot(ext[,2]~N)
```

We can predict importation rates from fitted models

```{r matrices}
#Gravity
readRDS("gravity-mle.rds") %$% {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  (N^tau1)*(theta*t(Q))
} -> GRmat

#Xia
readRDS("xia-mle.rds") %$% {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  (N^tau1)*(theta*t(Q))
} -> XImat

#CD
readRDS("compdest-mle.rds") %$% {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  (N^tau1)*(theta*t(Q*R))
} -> CDmat

#Stouffer
readRDS("stouffer-mle.rds") %$% {
  readRDS("rankmat.rds") -> rr
  theta <- exp(theta)
  phi <- exp(phi)
  (N^tau1)*(theta*((rr^tau2)))
} -> STmat

#Stouffer variant
readRDS("stouffer1-mle.rds") %$% {
  readRDS("rankmat1.rds") -> rr
  theta <- exp(theta)
  phi <- exp(phi)
  (N^tau1)*(theta*((rr^tau2)))
} -> SVmat

#Radiation
mle.radiation %$% {
  readRDS("radmat.rds") -> rr
  theta <- exp(theta)
  theta*rr
} -> RDmat

#Radiation Variant
mle.radiation1 %$% {
  readRDS("radmat1.rds") -> rr
  theta <- exp(theta)
  theta*rr
} -> RVmat
```

```{r import_export,fig.height=6.83}
library(tidyr)

data.frame(
  SV=SVmat %>% rowMeans(),
  ST=STmat %>% rowMeans(),
  GR=GRmat %>% rowMeans(),
  CD=CDmat %>% rowMeans(),
  N=N
) %>% 
  gather(model,imports,-N) %>%
  ggplot(aes(x=N,y=imports/N,color=model))+
  geom_point(alpha=1)+scale_x_log10()+scale_y_log10()+
  facet_grid(model~.)

data.frame(
  SV=SVmat %>% colMeans(),
  ST=STmat %>% colMeans(),
  GR=GRmat %>% colMeans(),
  CD=CDmat %>% colMeans(),
  N=N
) %>% 
  gather(model,exports,-N) %>%
  ggplot(aes(x=N,y=exports/N,color=model))+
  geom_point(alpha=1)+scale_x_log10()+scale_y_log10()+
  facet_grid(model~.)
```

We next create the fade-out plot
```{r fadeout-plot,fig.height=6.83}
dat$E=dat$cases==0 
dat %>% ddply(~town,summarize,mean(E)) -> ext

par(mfrow=c(1,1))
par(mar = c(5,5,2,5))
plot(ext[,2]~N, log="x", xlab="Pop.size", ylab="Presence")

par(new=T)

plot(log(ext[,2]/(1-ext[,2]))~N, log="x",type="p", col="red", axes=FALSE, xlab=NA, ylab=NA)
axis(side = 4)
mtext(side = 4, line = 4, "logit")
legend("topright",
  legend=c("Presence", "logit"),
  pch=c(21,21),
  col=c("black", "red"))
```


# Next steps

1. Debug loglikelihood for Xia and gravity models
2. Include intercept term to capture background
1. Use negative binomial model
3. Compare with "null" models (diffusion, independent)
4. Model-model comparison of coupling matrices (which places are the strongest outliers) using likelihood residuals

Plots:

1. empirical CCS plot with residuals (how well do models capture the residuals?)
1. likelihood comparison maps

1. Expontial distance kernels
1. qAICs and qBayes-weights in table 
1. Solve optimization issues
1. Random effects.
1. Zero-inflated or negative binomial model?
1. Revisit $\sigma$ profile
1. Revisit TSIR fitting (include residuals?)
1. Include references for the various models.

# References
