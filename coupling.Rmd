---
title: "Spatial coupling in measles via fade-out survival analysis"
output:
  html_document:
    theme: default
    toc: yes
    toc_depth: 2
bibliography: gravity.bib
csl: ecology.csl
---

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta[1]{{\Delta}{#1}}
\newcommand\lik{\mathscr{L}}
\newcommand\loglik{\ell}

Produced with **R** version `r getRversion()`.

--------------------------

```{r knitr-opts,include=FALSE,purl=FALSE,cache=FALSE}
library(knitr)
prefix <- "coupling"
opts_chunk$set(
  eval=TRUE,
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  cache.extra=rand_seed,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
  )
```
```{r prelims,include=FALSE,cache=FALSE}
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  encoding="UTF-8"
  )

set.seed(594709947L)
require(ggplot2)
theme_set(theme_bw())
require(grid)
require(plyr)
require(reshape2)
require(magrittr)
require(foreach)
require(bbmle)
require(pomp)

mpi <- !is.na(as.integer(Sys.getenv("OMPI_COMM_WORLD_SIZE")))
if (mpi) {
    require(doMPI)
    cl <- startMPIcluster(bcast=FALSE)
    registerDoMPI(cl)
} else {
    require(doMC)
    registerDoMC()
}
```

# Preparations

## Preprocess the data.

Retrieve the data from the database.

```{r clean-data,purl=FALSE}
bake(file="ew_measles_data.rds",{
  require(aakmisc)
  options(aakmisc.dbname="ewmeasles",aakmisc.remotehost="kinglab.eeb.lsa.umich.edu")
  
  startTunnel()
  
  getQuery("select town,year,births,pop from demog where year>=1944 order by town,year") -> demog

  getQuery("select town,date,cases from measles where year>=1944 order by town,date") %>%
    mutate(year=as.integer(format(date+3,"%Y"))) %>%
    ddply(~town+year,mutate,week=seq_along(year),biweek=(week+1)%/%2) %>%
    subset(week<=52,select=-c(date,week)) %>%
    acast(town~year~biweek,value.var="cases",fun.aggregate=sum) %>%
    melt(varnames=c("town","year","biweek"),value.name="cases") %>%
    mutate(town=as.character(town)) %>%
    arrange(town,year,biweek) %>%
    join(demog,by=c("town","year")) %>%
    mutate(births=births/26) -> measles
  
  stopTunnel()
  measles
}) -> dat
```

In the above:

1. Measles data is weekly; dates given correspond to Sunday.
1. Associate each week's data with the Weds. of that week.
1. Discard any 53rd weeks (1947,1952,1958,1964).
1. Aggregate by biweek (26 biweeks/yr).
1. Join with demographic data.
1. Scale birth rates to births/biweek.

## Apply TSIR model to the individual towns.

### Susceptible reconstruction.

Smoothing spline regression of cumulative cases on cumulative births to estimate under-reporting and residuals.
Correct for under-reporting: `I = cases/ur`.

Regress cumulative cases on cumulative births to estimate under-reporting (ur) and susceptible depletion (`z`) `I` is estimated as `cases/ur`.

```{r under-reporting,purl=FALSE}
estUR <- function (dat, df = 2.5) {
  cumbirths <- cumsum(dat$births)
  cumcases <- cumsum(dat$cases)
  fit <- smooth.spline(cumbirths,cumcases,df=df)
  mutate(dat,
         ur=predict(fit,x=cumbirths,deriv=1)$y,
         I=cases/ur)
}

dat %<>% ddply(~town,estUR,.parallel=TRUE)
```

Now reconstruct the susceptibles (via the residuals `z`).

```{r susceptible-reconstruction,purl=FALSE}
suscRecon <- function (dat, df = 2.5) {
  cumbirths <- cumsum(dat$births)
  cuminc <- cumsum(dat$I)
  fit <- smooth.spline(cumbirths,cuminc,df=df)
  mutate(dat,z=-residuals(fit))
}

dat %<>% ddply(~town,suscRecon,.parallel=TRUE)
```

*Does the preceding scale the z variable appropriately?*
*I.e., does it take under-reporting into account?*

### Fit TSIR

First, set up the data matrix for the regression.
This involves lagging the I and z variables.
Population size is assumed constant at its median value.

```{r make-data-matrix,purl=FALSE}
dat %<>%
  ddply(~town,mutate,
        Ilag=c(NA,head(I,-1)),
        zlag=c(NA,head(z,-1)),
        Ps=median(pop),
        seas=factor(biweek,levels=1:26)) %>%
  ddply(~town,tail,-1)
```

We'll start by estimating the mean susceptible fraction, assuming a single global value for this parameter.
Some towns have very high changes in susceptible fraction; exclude these.

```{r exclude-weird-towns,purl=FALSE}
dat %>%
  ddply(~town,summarize,
        maxfrac=max(-z/Ps)) %>%
  subset(maxfrac>0.03) %>%
  extract2("town") -> excludes

print(length(excludes))
```

Now profile on the fraction, $\sigma$, of susceptibles in the population.
This assumes a single, global value of $\sigma$.
**NOTE: the $\beta$ are here scaled by population size.**

```{r sigma-profile1,purl=FALSE}
dat %>% subset(!(town%in%excludes)) -> dat1

sigma1dev <- function (dat, sigma) {
  slag <- with(dat,sigma*Ps+zlag)
  fit <- glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(slag)),
             data=dat,subset=Ilag>0&I>0)
  fit$deviance
}

foreach (sigma=seq(0.03,0.25,length=100),
         .combine=rbind,.inorder=FALSE) %do%
         {
           dat1 %>% 
             daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>% 
             sum() -> dev
           data.frame(sigma=sigma,dev=dev)
         } %>%
  ggplot(aes(x=sigma,y=dev))+geom_line()
```

```{r sigma-profile2,purl=FALSE}
fit <- optim(par=0.037,lower=0.03,upper=0.10,
             method="Brent",hessian=TRUE,
             fn=function (sigma) {
               dat1 %>% 
                 daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>% 
                 sum()
             })
sigmaBar <- fit$par

dat %<>% mutate(Slag=sigmaBar*Ps+zlag)
```

Our global sigma estimate is $\sigma=`r signif(sigmaBar,3)`$.
Now, we assume this value of $\sigma$ and fit TSIR to each of the towns individually using linear regression.

```{r fit-tsir}
bake(file="tsir-fits.rds",{
  coefnames <- c(sprintf("seas%d",1:26),"log(Ilag)","I(1/Ilag)")
  newcoefnames <- c(sprintf("log.beta%02d",1:26),"alpha","m.alpha")
  
  tsirfit <- function (dat) {
    glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(Slag)),
        data=dat,subset=Ilag>0&I>0) %>% summary() %>% 
      extract2("coefficients") -> fit
    fit[,"Estimate"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(newcoefnames) -> coefs
    fit[,"Std. Error"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(paste(newcoefnames,"se",sep=".")) -> se
    cbind(coefs,se,sigma=sigmaBar,
          town=unique(dat$town),Ps=unique(dat$Ps))
  }
  
  dat %>% ddply(~town,tsirfit,.parallel=TRUE) %>% 
    melt(id=c("town","Ps")) %>% 
    mutate(se=ifelse(grepl("\\.se$",variable),"se","est"),
           variable=sub("\\.se$","",variable)) %>%
    dcast(town+Ps+variable~se) -> tsirs
  
  dat %>% ddply(~town,summarize,Ps=unique(Ps)) -> tsircoef
  tsirs %>% 
    na.omit() %>%
    ddply(~variable, function (d) {
      fit <- lm(est~log(Ps),data=d,weights=1/se^2) 
      data.frame(town=tsircoef$town,value=predict(fit,newdata=tsircoef))
    },.parallel=TRUE) %>% 
    dcast(town~variable) %>% 
    melt(id=c("town","alpha","m.alpha"),value.name="log.beta") %>%
    mutate(biweek=as.integer(sub("log.beta","",as.character(variable)))) %>%
    arrange(town,biweek) %>%
    subset(select=-variable) -> tsircoef
  
  dat %<>% join(tsircoef,by=c("town","biweek"))
  
  dat %<>% mutate(ylag=Ilag/Ps)
}) -> dat
```

Just for interest, let's plot $R_0$ as a function of city size.
```{r R0-plot,purl=FALSE}
require(scales)
dat %>% ddply(~town,summarize,N=mean(Ps),
              beta=exp(mean(log.beta)),R0=beta*N) %>%
    ggplot(aes(x=N,y=R0))+geom_point()+
    scale_x_log10(breaks=10^seq(2,7),
                  labels=trans_format('log10',math_format(10^.x)))+
    scale_y_log10(breaks=seq(1,30))+
    labs(x="Population size",y=expression(R[0]))+
    theme_classic()
```

Now let's compute the distances between the cities.
```{r distances}
bake(file="distances.rds",{
  require(aakmisc)
  options(aakmisc.dbname="ewmeasles",aakmisc.remotehost="kinglab.eeb.lsa.umich.edu")
  
  startTunnel()
  getQuery("select * from coords") %>%
    arrange(town) -> coords
  stopTunnel()
  rownames(coords) <- coords$town
  
  require(ncf)
   
  ## compute distance matrix
  expand.grid(i=coords$town,j=coords$town) %>%
    ddply(~i,mutate,
          dist=mapply(gcdist,x1=coords[i,"long"],y1=coords[i,"lat"],
                      x2=coords[j,"long"],y2=coords[j,"lat"]),
          .parallel=TRUE) %>% 
    # mutate(dist=mapply(dist,i,j)) %>% 
    acast(i~j,value.var="dist")
}) -> distances
```

- `Shat` is fitted to each individually.
- `Sbar` is fitted globally (with some towns excluded).

# Coupling the cities.

Let $X_{i}(t)$ be the observation at time $t$ in city $i$.
We assume that
$X_{i}(t+1) \sim \dist{Poisson}{\lambda_i(t)}$.

When $I_i(t)=0$, we have that
$$\lambda_i(t) = \beta_i(t)\,S_i(t)\,\iota_i(t)^\alpha.$$
In the above, $\beta$ is constructed by fitting the TSIR model to each city independently and the susceptible pool, $S$, is reconstructed using TSIR methods.
The quantity $\iota$ is the import rate, which we estimate using a variety of different models.


# The gravity model

The gravity model is
$$\iota_i=\theta N_i^{\tau_1} \sum_{j\ne i}\! N_j^{\tau_2} d_{ij}^{-\rho}\,\frac{I_j}{N_j}.$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the gravity model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y.$$


The following computes $y$, $S$, $\beta$, and the matrix of reciprocal distances.
It also picks out the relevant observations.
These are the ones for which the preceding week saw zero cases.
```{r gravity-pre}
dat %>% acast(town~year+biweek,value.var="ylag") -> ylag
dat %>% acast(town~year+biweek,value.var="Slag") -> slag
dat %>% acast(town~year+biweek,value.var="log.beta") %>% exp() -> beta
dat %>% acast(town~year+biweek,value.var="cases") -> obs
dat %>% daply(~town,function(x)unique(x$Ps)) -> N

dd <- 1/distances
diag(dd) <- 0

stopifnot(identical(rownames(ylag),rownames(slag)))
stopifnot(identical(rownames(ylag),rownames(beta)))
stopifnot(identical(rownames(ylag),rownames(obs)))
stopifnot(identical(rownames(ylag),names(N)))
stopifnot(identical(rownames(ylag),rownames(dd)))
stopifnot(identical(rownames(ylag),colnames(dd)))
      
ddscal <- exp(mean(log(dd[lower.tri(dd)])))
dd <- dd/ddscal

alpha <- mean(dat$alpha)

relevant <- which(ylag==0&slag>=0)
obs <- obs[relevant]
betaS <- beta[relevant]*slag[relevant]
```

The negative log likelihood function for the gravity model is:
```{r gravity-like}
likfnGravity <- function (theta, rho, tau1, tau2) {
  theta <- exp(theta)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- theta*(N^tau1)*crossprod(Q,ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

Now we test the likelihood function by attempting to maximize it.
```{r gravity-test,purl=FALSE}
par <- list(theta=-8, rho=0, tau1=0.5, tau2=0.5)
with(par,likfnGravity(theta,rho,tau1,tau2))

bake(file="gravity-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnGravity,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=1000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test

summary(test)

with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),tau1=tau1,tau2=tau2))

vcov(test)
cov2cor(vcov(test))
```

```{r gravity-profile-2D}
bake(file="gravity.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.4, length=25), 
    tau2=seq(-1, 1, length=25),
    rho=log(1.0),
    theta=log(0.0001)
  )
  
  tic <- Sys.time()
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .options.mpi=list(seed=95886868,chunkSize=1),
          .packages="bbmle"
  ) %do% try(
    {
      mle2(likfnGravity,
           method="Nelder-Mead",
           start=as.list(start[c("rho","theta")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=1000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  toc <- Sys.time()
  print(toc-tic)
  
  results
}) -> results
```

```{r gravity-results,purl=FALSE}
results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique()

results %<>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() 

results %>%
    ggplot(aes(x=tau1,y=tau2,z=loglik,
               fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
    geom_tile(color=NA)+geom_contour(bins=100,color='white')+
    labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))

results %>%
    subset(loglik==max(loglik)) -> mle.grav
```

```{r gravity-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+rho+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# The Xia (wonky gravity) model

The model of @Xia2004 is
$$\iota_i=\theta N_i^{\tau_1} \sum_{j\ne i}\! I_j^{\tau_2} d_{ij}^{-\rho}.$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the @Xia2004 model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y^{\tau_2}.$$

The negative log likelihood function for the @Xia2004 model is:
```{r xia-like}
likfnXia <- function (theta, rho, tau1, tau2) {
  theta <- exp(theta)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- theta*(N^tau1)*crossprod(Q,ylag^tau2)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

Now we test the likelihood function by attempting to maximize it.
```{r xia-test,purl=FALSE}
par <- list(theta=-8, rho=0, tau1=0.5, tau2=0.5)
with(par,likfnXia(theta,rho,tau1,tau2))

bake(file="xia-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnXia,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=1000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test

summary(test)

with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),tau1=tau1,tau2=tau2))

vcov(test)
cov2cor(vcov(test))
```

```{r xia-profile-2D}
bake(file="xia.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.5, length=25), 
    tau2=seq(0.1, 1.0, length=25),
    rho=log(1.0),
    theta=log(1e-6)
  )
  
  tic <- Sys.time()
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .options.mpi=list(seed=95886868,chunkSize=1),
          .packages="bbmle"
  ) %do% try(
    {
      mle2(likfnXia,
           method="Nelder-Mead",
           start=as.list(start[c("rho","theta")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=1000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  toc <- Sys.time()
  print(toc-tic)
  
  results
}) -> results
```

```{r xia-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
    ldply() 

results %>%
    ggplot(aes(x=tau1,y=tau2,z=loglik,
               fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
    geom_tile(color=NA)+geom_contour(bins=100,color='white')+
    labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))

results %>%
    subset(loglik==max(loglik)) -> mle.xia
```

```{r xia-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+rho+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# Competing destinations model

The competing destinations model is
$$\iota_i=\theta N_i^{\tau_1} \sum_j\frac{N_j^{\tau_2}}{d_{ij}^\rho}\,\left(\sum_{k \ne i, j}\frac{N_k^{\tau_2}}{d_{jk}^\rho}\right)^\delta\,\frac{I_j}{N_j}$$
note that the $\tau_2$ and $\rho$ coefficients are the same as in the gravity model, so the whole thing would be

Let
$$Q_{ij}=\begin{cases}{N_i^{\tau_2}}{d_{ij}^{-\rho}}, &i\ne j\\0, &i=j\end{cases}$$ and
$$R_{ji}=\sum_{k \ne i, j}{N_k^{\tau_2}}{d_{jk}^{-\rho}}=\sum_{k\ne i,j}Q_{kj}=\sum_{k\ne i}Q_{kj}=\sum_{k}Q_{kj}-Q_{ij}.$$
This implies
$$R^T=(\mathbb{1}\,\mathbb{1}^T-I)\,Q \qquad \Longleftrightarrow \qquad R=Q^T\,(\mathbb{1}\,\mathbb{1}^T-I)$$
and
$$\iota_i=\theta\,N_i^{\tau_1}\,\sum_j Q_{ji}\,R_{ji}^\delta\,y_{j}.$$

```{r compdest-pre}
iii <- 1-diag(length(N))
```

The negative log likelihood function for the competing destinations model is:
```{r compdest-like}
likfnCompDest <- function (theta, rho, tau1, tau2, delta) {
  theta <- exp(theta)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  iota <- theta*(N^tau1)*crossprod(Q*R,ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

Now we test the likelihood function by attempting to maximize it.
```{r compdest-test,purl=FALSE}
par <- list(theta=0, rho=0, tau1=0.5, tau2=0.5, delta=0)
with(par,likfnCompDest(theta,rho,tau1,tau2,delta))

bake(file="compdest-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnCompDest,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=4000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test

summary(test)

with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),delta=delta,tau1=tau1,tau2=tau2))

vcov(test)
cov2cor(vcov(test))
```

```{r compdest-profile-2D}
bake(file="compdest.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    rho=log(1.0),
    theta=log(1),
    delta=0
  )
  
  tic <- Sys.time()
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .options.mpi=list(seed=95886868,chunkSize=1),
          .packages="bbmle"
  ) %do% try(
    {
      mle2(likfnCompDest,
           method="Nelder-Mead",
           start=as.list(start[c("rho","theta","delta")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=4000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  toc <- Sys.time()
  print(toc-tic)
  
  results
}) -> results
```

```{r compdest-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() 

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
             fill=ifelse(loglik>max(loglik)-2000,
                         loglik,NA)))+
    geom_tile(color=NA)+geom_contour(bins=100,color='white')+
    labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))

results %>%
    subset(loglik==max(loglik)) -> mle.compdest
```

```{r compdest-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+rho+delta+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# Wonky competing destinations

The negative log likelihood function for the wonky competing destinations model is:

```{r wonk-compdest-like}
likfnWonkCompDest <- function (theta, rho, tau1, tau2, delta) {
  theta <- exp(theta)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  iota <- theta*(N^tau1)*crossprod(Q*R,ylag^tau2)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

Now we test the likelihood function by attempting to maximize it.

```{r wonk-compdest-test,purl=FALSE}
par <- list(theta=0, rho=0, tau1=0.5, tau2=0.5, delta=0)
with(par,likfnCompDest(theta,rho,tau1,tau2,delta))

bake(file="wonkcompdest-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnWonkCompDest,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=4000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test

summary(test)

with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),delta=delta,tau1=tau1,tau2=tau2))

vcov(test)
cov2cor(vcov(test))
```

```{r wonk-compdest-profile-2D}
bake(file="wonkcompdest.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    rho=log(1.0),
    theta=log(1),
    delta=0
  )
  
  tic <- Sys.time()
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .options.mpi=list(seed=95886868,chunkSize=1),
          .packages="bbmle"
  ) %do% try(
    {
      mle2(likfnWonkCompDest,
           method="Nelder-Mead",
           start=as.list(start[c("rho","theta","delta")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=4000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  toc <- Sys.time()
  print(toc-tic)
  
  results
}) -> results
```

```{r wonk-compdest-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() 

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
             fill=ifelse(loglik>max(loglik)-2000,
                         loglik,NA)))+
    geom_tile(color=NA)+geom_contour(bins=100,color='white')+
    labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))

results %>%
    subset(loglik==max(loglik)) -> mle.wonkcompdest
```

```{r wonk-compdest-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+rho+delta+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# Stouffer model

Let $i$ be the recipient town; $j$, the donor.
Let $S(i,j)$ be the collection of towns closer to town $i$ than $j$ is.
That is, $S(i,j) = \{k: d(i,k) < d(i,j)\}$.

$$\iota_i = \theta\,N_i^{\tau_1}\,\sum_j\!\left(\frac{N_j}{\sum_{k\in S(i,j)}{N_k}}\right)^{\tau_2}\,\frac{I_j}{N_j}$$


```{r rankmat}
bake(file="rankmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/sum(N[distances[i,]<distances[i,j]])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r stouffer-like}
likfnStouffer <- function (theta, tau1, tau2) {
  theta <- exp(theta)
  iota <- theta*(N^tau1)*((rr^tau2)%*%ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

```{r stouffer-test,purl=FALSE}
par <- list(theta=0, tau1=0.5, tau2=0.5)
with(par,likfnStouffer(theta,tau1,tau2))

bake(file="stouffer-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnStouffer,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=4000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test

summary(test)

with(as.list(coef(test)),c(exp(c(theta=theta)),tau1=tau1,tau2=tau2))

vcov(test)
cov2cor(vcov(test))
```

```{r stouffer-profile-2D}
bake(file="stouffer.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    theta=log(1)
  )
  
  tic <- Sys.time()
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .options.mpi=list(seed=95886868,chunkSize=1),
          .packages="bbmle"
  ) %do% try(
    {
      mle2(likfnStouffer,
           method="Brent",
           lower=-20,upper=20,
           start=as.list(start[c("theta")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=4000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  toc <- Sys.time()
  print(toc-tic)
  
  results
}) -> results
```

```{r stouffer-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(is.finite(loglik))

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
             fill=ifelse(loglik>max(loglik,na.rm=TRUE)-2000,
                         loglik,NA)))+
    geom_tile(color=NA)+geom_contour(bins=100,color='white')+
    labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))

results %>%
    subset(loglik==max(loglik)) -> mle.stouffer
```

```{r stouffer-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+tau1+tau2,
      data=results,
      subset=loglik>max(loglik,na.rm=TRUE)-10000,cex=0.5)
```

# Model comparison

```{r modelcomp,cache=FALSE,echo=FALSE}
list(gravity=mle.grav,xia=mle.xia,comp.dest=mle.compdest,
     wonky.comp.dest=mle.wonkcompdest,stouffer=mle.stouffer) %>%
    ldply(.id="model") %>%
    arrange(-loglik) %>%
    kable(digits=3)
```

```{r include=FALSE,eval=FALSE,purl=FALSE}
dist=distSel
diag(dist)=Inf

rankMat=matrix(0, ncol=dim(distSel)[2], nrow=dim(distSel)[1])

for(i in 1:952){
  for(j in 1:952){
    rankMat[i,j]=PuSel[j]/sum(PuSel[dist[i,j]>dist[i,]])
  }}

plot(rankMat[1,], dist[1,], log='xy')

diag(rankMat)=0

rankMat[!is.finite(rankMat)]=0

par <- c(theta=-15, tau1=0, tau2=-1)

ySel <- t(MuSel)/PuSel
fadeout <- ySel==0
S <- t(SuSel)
beta <- t(exp(betaSel))

relevant <- which(fadeout[,-546])
obs <- fadeout[,-1][relevant]

likfn2 <- function (theta, tau1, tau2) {
  theta <- exp(theta)
  tau1<-exp(tau1)
  tau2<-exp(tau2)
  iotaSel <- theta*(PuSel^tau1)*(rankMat^tau2%*%ySel)
  lambda <- beta*S*iotaSel^alpha
  prob <- exp(-lambda[relevant])
  #print(sum(!is.finite(prob)))
  -sum(log(ifelse(obs,prob,1-prob)))
}

test2<-mle2(likfn2, start=list(theta=-1, tau1=-3, tau2=-0), control=list(trace=4), method='Nelder-Mead')
ctest2<-confint(test2)
ptest2<-profile(test2)
```


# Next steps

1. Zero-inflated or negative binomial model?
1. Solve optimization issues
1. Revisit $\sigma$ profile
1. Revisit TSIR fitting (include residuals?)
1. Include references for the various models.

```{r stop-mpi,include=FALSE}
if (mpi) {
    closeCluster(cl)
    try(detach("package:doMPI",unload=TRUE),silent=TRUE)
    mpi.exit()
    try(detach("package:Rmpi",unload=TRUE),silent=TRUE)
}
```

# References
