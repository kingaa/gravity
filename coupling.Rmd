---
title: "Spatial coupling in measles via fade-out survival analysis"
csl: ecology.csl
output:
  html_document:
    code_folding: hide
    df_print: kable
    highlight: tango
    dev: png
    number_sections: no
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
    toc_depth: 2
    dev: png
    df_print: kable
bibliography: gravity.bib
---

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta[1]{{\Delta}{#1}}
\newcommand\lik{\mathscr{L}}
\newcommand\loglik{\ell}

Produced with **R** version `r getRversion()`.

--------------------------

```{r knitr-opts,include=FALSE,purl=FALSE,cache=FALSE}
library(knitr)
prefix <- "coupling"
opts_chunk$set(
  eval=TRUE,
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  cache.extra=rand_seed,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
  )
```

# Preparations

```{r prelims,cache=FALSE}
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  knitr.kable.NA="",
  encoding="UTF-8",
  aakmisc.dbname="ewmeasles",
  aakmisc.remotehost="kinglab.eeb.lsa.umich.edu",
  aakmisc.user="gravity")

set.seed(594709947L)

library(grid)
library(plyr)
library(reshape2)
library(magrittr)
library(foreach)
library(iterators)
library(bbmle)
library(pomp)
library(ggplot2)
theme_set(theme_bw())

mpi <- !is.na(as.integer(Sys.getenv("OMPI_COMM_WORLD_SIZE")))
if (mpi) {
    library(doMPI)
    cl <- startMPIcluster(bcast=FALSE)
    registerDoMPI(cl)
} else {
    library(doMC)
    registerDoMC()
}
```

## Preprocess the data.

Retrieve the data from the database.

```{r clean-data,purl=FALSE,cache=FALSE}
bake(file="ew_measles_data.rds",{
  library(aakmisc)
  
  startTunnel()
  
  getQuery("select town,year,births,pop from demog where year>=1944 order by town,year") -> demog

  getQuery("select town,date,cases from measles where year>=1944 order by town,date") %>%
    mutate(year=as.integer(format(date+3,"%Y"))) %>%
    ddply(~town+year,mutate,week=seq_along(year),biweek=(week+1)%/%2) %>%
    subset(week<=52,select=-c(date,week)) %>%
    acast(town~year~biweek,value.var="cases",fun.aggregate=sum) %>%
    melt(varnames=c("town","year","biweek"),value.name="cases") %>%
    mutate(town=as.character(town)) %>%
    arrange(town,year,biweek) %>%
    join(demog,by=c("town","year")) %>%
    mutate(births=births/26) -> measles
  
  stopTunnel()
  measles
}) -> dat
```

In the above:

1. Measles data is weekly; dates given correspond to Sunday.
1. Associate each week's data with the Weds. of that week.
1. Discard any 53rd weeks (1947,1952,1958,1964).
1. Aggregate by biweek (26 biweeks/yr).
1. Join with demographic data.
1. Scale birth rates to births/biweek.

Now let's compute the distances between the cities.

```{r distances,cache=FALSE}
bake(file="distances.rds",{
  library(aakmisc)
  
  startTunnel()
  getQuery("select * from coords") %>% arrange(town) -> coords
  stopTunnel()
  rownames(coords) <- coords$town
  
  library(ncf)
   
  ## compute distance matrix
  expand.grid(i=coords$town,j=coords$town) %>%
    ddply(~i,mutate,
          dist=mapply(gcdist,x1=coords[i,"long"],y1=coords[i,"lat"],
                      x2=coords[j,"long"],y2=coords[j,"lat"]),
          .parallel=TRUE) %>% 
    # mutate(dist=mapply(dist,i,j)) %>% 
    acast(i~j,value.var="dist")
}) -> distances
```

## Apply TSIR model to the individual towns.

### Susceptible reconstruction.

Smoothing spline regression of cumulative cases on cumulative births to estimate under-reporting and residuals.
Correct for under-reporting: `I = cases/ur`.

Regress cumulative cases on cumulative births to estimate under-reporting (ur) and susceptible depletion (`z`) `I` is estimated as `cases/ur`.

```{r under-reporting,purl=FALSE}
estUR <- function (dat, df = 2.5) {
  cumbirths <- cumsum(dat$births)
  cumcases <- cumsum(dat$cases)
  fit <- smooth.spline(cumbirths,cumcases,df=df)
  mutate(dat,
         ur=predict(fit,x=cumbirths,deriv=1)$y,
         I=cases/ur)
}

dat %>% ddply(~town,estUR,.parallel=TRUE) -> dat
```

Now reconstruct the susceptibles (via the residuals `z`).

```{r susceptible-reconstruction,purl=FALSE}
suscRecon <- function (dat, df = 2.5) {
  cumbirths <- cumsum(dat$births)
  cuminc <- cumsum(dat$I)
  fit <- smooth.spline(cumbirths,cuminc,df=df)
  mutate(dat,z=-residuals(fit))
}

dat %>% ddply(~town,suscRecon,.parallel=TRUE) -> dat
```

*Does the preceding scale the z variable appropriately?*
*I.e., does it take under-reporting into account?*

### Fit TSIR

First, set up the data matrix for the regression.
This involves lagging the I and z variables.
Population size is assumed constant at its median value.

```{r make-data-matrix,purl=FALSE}
dat %>%
  ddply(~town,mutate,
        Ilag=c(NA,head(I,-1)),
        zlag=c(NA,head(z,-1)),
        Ps=median(pop),
        seas=factor(biweek,levels=1:26)) %>%
  ddply(~town,tail,-1) -> dat
```

We'll start by estimating the mean susceptible fraction, assuming a single global value for this parameter.
Some towns have very high changes in susceptible fraction; exclude these.

```{r exclude-weird-towns,purl=FALSE}
dat %>%
  ddply(~town,summarize,
        maxfrac=max(-z/Ps)) %>%
  subset(maxfrac>0.03) %>%
  extract2("town") -> excludes

print(length(excludes))
```

Now profile on the fraction, $\sigma$, of susceptibles in the population.
This assumes a single, global value of $\sigma$.
**NOTE: the $\beta$ are here scaled by population size.**

```{r sigma-profile,purl=FALSE,cache=FALSE}
stew("sigma-profile.rda",{
    
    dat %>% subset(!(town%in%excludes)) -> dat1
    
    sigma1dev <- function (dat, sigma) {
        slag <- with(dat,sigma*Ps+zlag)
        fit <- glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(slag)),
                   data=dat,subset=Ilag>0&I>0)
        fit$deviance
    }
    
    foreach (sigma=seq(0.03,0.25,length=100),
             .combine=rbind,.inorder=FALSE) %dopar%
        {
            dat1 %>% 
                daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>% 
                sum() -> dev
            data.frame(sigma=sigma,dev=dev)
        } -> sigmaProf
    
    fit <- optim(par=0.037,lower=0.03,upper=0.10,
                 method="Brent",hessian=TRUE,
                 fn=function (sigma) {
                     dat1 %>% 
                         daply(~town,sigma1dev,sigma=sigma,.parallel=TRUE) %>% 
                         sum()
                 })
    fit$par -> sigmaBar

    rm(dat1)

    dat %>% mutate(Slag=sigmaBar*Ps+zlag) -> dat
})

sigmaProf %>%
    ggplot(aes(x=sigma,y=dev))+geom_line()
```

Our global sigma estimate is $\sigma=`r signif(sigmaBar,3)`$.
Now, we assume this value of $\sigma$ and fit TSIR to each of the towns individually using linear regression.

```{r fit-tsir,cache=FALSE}
bake(file="tsir-fits.rds",{
  coefnames <- c(sprintf("seas%d",1:26),"log(Ilag)","I(1/Ilag)")
  newcoefnames <- c(sprintf("log.beta%02d",1:26),"alpha","m.alpha")
  
  tsirfit <- function (dat) {
    glm(log(I)~-1+seas+log(Ilag)+I(1/Ilag)+offset(log(Slag)),
        data=dat,subset=Ilag>0&I>0) %>% summary() %>% 
      extract2("coefficients") -> fit
    fit[,"Estimate"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(newcoefnames) -> coefs
    fit[,"Std. Error"] %>% extract(coefnames) %>% as.list() %>% as.data.frame() %>%
      set_names(paste(newcoefnames,"se",sep=".")) -> se
    cbind(coefs,se,sigma=sigmaBar,
          town=unique(dat$town),Ps=unique(dat$Ps))
  }
  
  dat %>% ddply(~town,tsirfit,.parallel=TRUE) %>% 
    melt(id=c("town","Ps")) %>% 
    mutate(se=ifelse(grepl("\\.se$",variable),"se","est"),
           variable=sub("\\.se$","",variable)) %>%
    dcast(town+Ps+variable~se) -> tsirs
  
  dat %>% ddply(~town,summarize,Ps=unique(Ps)) -> tsircoef
  
  tsirs %>% 
    na.omit() %>%
    ddply(~variable, function (d) {
      fit <- lm(est~log(Ps),data=d,weights=1/se^2) 
      data.frame(town=tsircoef$town,value=predict(fit,newdata=tsircoef))
    },.parallel=TRUE) %>% 
    dcast(town~variable) %>% 
    melt(id=c("town","alpha","m.alpha"),value.name="log.beta") %>%
    mutate(biweek=as.integer(sub("log.beta","",as.character(variable)))) %>%
    arrange(town,biweek) %>%
    subset(select=-variable) -> tsircoef
  
  dat %>%
    join(tsircoef,by=c("town","biweek")) %>%
    mutate(ylag=Ilag/Ps) -> dat
}) -> dat
```

Just for interest, let's plot $R_0$ as a function of city size.

```{r R0-plot,purl=FALSE}
library(scales)
dat %>% ddply(~town,summarize,N=mean(Ps),
              beta=exp(mean(log.beta)),R0=beta*N) %>%
    ggplot(aes(x=N,y=R0))+geom_point()+
    scale_x_log10(breaks=10^seq(2,7),
                  labels=trans_format('log10',math_format(10^.x)))+
    scale_y_log10(breaks=seq(1,30))+
    labs(x="Population size",y=expression(R[0]))+
    theme_classic()
```

- `Shat` is fitted to each individually.
- `Sbar` is fitted globally (with some towns excluded).

# Coupling the cities.

Let $X_{i}(t)$ be the observation at time $t$ in city $i$.
We assume that
$$X_{i}(t+1) \sim \dist{Poisson}{\lambda_i(t)}$$ 
or, alternatively,
$$X_{i}(t+1) \sim \dist{NegBinom}{\lambda_i(t),\frac{1}{\psi}},$$
where $\psi$ is an overdispersion parameter.
In the latter, we have parameterized the negative binomial distribution so that
$\expect{X_i(t+1)}=\lambda_i(t)$ and $\var{X_i(t+1)}=\lambda_i(t)+\psi\,\lambda_i(t)^2$.

When $I_i(t)=0$, we have that
$$\lambda_i(t) = \beta_i(t)\,S_i(t)\,\iota_i(t)^\alpha.$$
In the above, $\beta$ is constructed by fitting the TSIR model to each city independently and the susceptible pool, $S$, is reconstructed using TSIR methods.
The quantity $\iota$ is the import rate, which we estimate using a variety of different models.

The following computes $y$, $S$, $\beta$, and the matrix of reciprocal distances.
It also picks out the relevant observations.
These are the ones for which the preceding week saw zero cases.


```{r gravity-pre}
dat %>% acast(town~year+biweek,value.var="ylag") -> ylag
dat %>% acast(town~year+biweek,value.var="Slag") -> slag
dat %>% acast(town~year+biweek,value.var="log.beta") %>% exp() -> beta
dat %>% acast(town~year+biweek,value.var="cases") -> obs
dat %>% daply(~town,function(x)unique(x$Ps)) -> N

dd <- 1/distances
diag(dd) <- 0

stopifnot(identical(rownames(ylag),rownames(slag)))
stopifnot(identical(rownames(ylag),rownames(beta)))
stopifnot(identical(rownames(ylag),rownames(obs)))
stopifnot(identical(rownames(ylag),names(N)))
stopifnot(identical(rownames(ylag),rownames(dd)))
stopifnot(identical(rownames(ylag),colnames(dd)))
      
ddscal <- exp(mean(log(dd[lower.tri(dd)])))
dd <- dd/ddscal

alpha <- mean(dat$alpha)

relevant <- which(ylag==0&slag>=0)
obs <- obs[relevant]
betaS <- beta[relevant]*slag[relevant]
```

# The gravity model

The gravity model (with intercept) is
$$\iota_i=N_i^{\tau_1} \left(\theta\,\sum_{j\ne i}\! N_j^{\tau_2} d_{ij}^{-\rho}\,\frac{I_j}{N_j}+\phi\right).$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the gravity model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y+\phi\,N^{\tau_1}.$$

We use **R**'s element-recycling feature, which allows us to write
$$\mathrm{diag}(A)\,B=A\;*\;B.$$


The negative log likelihood function for the gravity model is:
```{r gravity-like}
likfnGravity <- function (theta, phi, rho, psi, tau1, tau2) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Now we'll do a likelihood profile over $\tau_1$ and $\tau_2$.

```{r gravity-profile-2D,cache=FALSE}
bake(file="gravity.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    rho=log(1.0),
    psi=log(5),
    theta=log(0.0001),
    phi=log(0.0001)
  )
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnGravity,
        method="Nelder-Mead",
        start=as.list(start[c("rho","theta","phi","psi")]),
        fixed=as.list(start[c("tau1","tau2")]), 
        control=list(trace=0,maxit=10000),
        skip.hessian=TRUE) -> fit
      
      c(coef(fit),loglik=as.numeric(logLik(fit)),
        conv=fit@details$convergence)
    }
  ) -> results

  attr(results,"nproc") <- getDoParWorkers()
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster.
Now we refine to obtain the global MLE.

```{r gravity-mle,purl=FALSE,cache=FALSE}
bake("gravity-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnGravity,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.grav
```

```{r gravity-results,purl=FALSE}
results %>%
    extract(sapply(results,inherits,"try-error")) %>%
    sapply(as.character) %>%
    unique()

results %<>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() 

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.grav)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.grav,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.grav,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r gravity-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+rho+psi+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# The Xia (wonky gravity) model

The model of @xia2004measles is
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_{j\ne i}\! I_j^{\tau_2} d_{ij}^{-\rho} + \phi\right).$$
Let
$$Q_{ij}=\begin{cases}N_i^{\tau_2}\,d_{ij}^{-\rho}, &i\ne j\\0, &i=j\end{cases}$$ and
$$y_{i}=\frac{I_i}{N_i}.$$

Expressed compactly, the @xia2004measles model is
$$\iota = \theta\,\mathrm{diag}(N^{\tau_1})\,Q^T\,y^{\tau_2}+\phi\,N^{\tau_1}.$$

The negative log likelihood function for the @xia2004measles model is:

```{r xia-like}
likfnXia <- function (theta, phi, rho, psi, tau1, tau2) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  iota <- (N^tau1)*(theta*crossprod(Q,ylag^tau2)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

Again, a profile computation.

```{r xia-profile-2D,cache=FALSE}
bake(file="xia.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.5, length=25), 
    tau2=seq(0.1, 1.0, length=25),
    psi=log(5),
    rho=log(1.0),
    theta=log(1e-6),
    phi=log(1e-6)
  )
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnXia,
           method="Nelder-Mead",
           start=as.list(start[c("rho","theta","psi","phi")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=10000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),
        conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r xia-mle,purl=FALSE,cache=FALSE}
bake("xia-mle.rds",{
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnXia,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.xia
```

```{r xia-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
    ldply() 

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.xia)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.xia,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.xia,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r xia-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+rho+psi+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# Competing destinations model

The competing destinations model is
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_j\frac{N_j^{\tau_2}}{d_{ij}^\rho}\,\left(\sum_{k \ne i, j}\frac{N_k^{\tau_2}}{d_{jk}^\rho}\right)^\delta\,\frac{I_j}{N_j}+\phi\right).$$

Let
$$Q_{ij}=\begin{cases}{N_i^{\tau_2}}{d_{ij}^{-\rho}}, &i\ne j\\0, &i=j\end{cases}$$ and
$$R_{ji}=\sum_{k \ne i, j}{N_k^{\tau_2}}{d_{jk}^{-\rho}}=\sum_{k\ne i,j}Q_{kj}=\sum_{k\ne i}Q_{kj}=\sum_{k}Q_{kj}-Q_{ij}.$$
This implies
$$R^T=(\mathbb{1}\,\mathbb{1}^T-I)\,Q \qquad \Longleftrightarrow \qquad R=Q^T\,(\mathbb{1}\,\mathbb{1}^T-I)$$
and
$$\iota_i=N_i^{\tau_1}\,\left(\theta\,\sum_j Q_{ji}\,R_{ji}^\delta\,y_{j}+\phi\right).$$

The negative log likelihood function for the competing destinations model is:

```{r compdest-like}
iii <- 1-diag(length(N))

likfnCompDest <- function (theta, phi, rho, psi, tau1, tau2, delta) {
  theta <- exp(theta)
  phi <- exp(phi)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  iota <- (N^tau1)*(theta*crossprod(Q*R,ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

We compute the profile likelihood as before.

```{r compdest-profile-2D,cache=FALSE}
bake(file="compdest.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    rho=log(0.6),
    theta=log(1),
    phi=log(0.001),
    psi=log(2),
    delta=-1
  )

  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnCompDest,
           method="Nelder-Mead",
           start=as.list(start[c("rho","theta","delta","psi","phi")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=10000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  attr(results,"nproc") <- getDoParWorkers()
  
  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r compdest-mle,purl=FALSE,cache=FALSE}
bake("compdest-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnCompDest,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.compdest
```


```{r compdest-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
    ldply() 

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.compdest)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.compdest,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.compdest,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r compdest-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+rho+delta+psi+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# Wonky competing destinations

The negative log likelihood function for the wonky competing destinations model is:

```{r wonk-compdest-like}
likfnWonkCompDest <- function (theta, rho, tau1, tau2, delta) {
  theta <- exp(theta)
  rho <- exp(rho)
  Q <- (N^tau2)*(dd^rho)
  R <- crossprod(Q,iii)^delta
  iota <- theta*(N^tau1)*crossprod(Q*R,ylag^tau2)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

Now we test the likelihood function by attempting to maximize it.

```{r wonk-compdest-test,purl=FALSE}
par <- list(theta=0, rho=0, tau1=0.5, tau2=0.5, delta=0)
with(par,likfnWonkCompDest(theta=theta,rho=rho,tau1=tau1,tau2=tau2,delta=delta))

bake(file="wonkcompdest-test.rds",{
    mle2(likfnWonkCompDest,
         method="Nelder-Mead",
         start=par,
         control=list(trace=0,maxit=4000))
}) -> test

summary(test)

with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),delta=delta,tau1=tau1,tau2=tau2))

vcov(test)
cov2cor(vcov(test))
```

```{r wonk-compdest-profile-2D,cache=FALSE}
bake(file="wonkcompdest.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.1, 1.4, length=25),
    tau2=seq(-1, 1, length=25),
    rho=log(1.0),
    theta=log(1),
    delta=0
  )
  
  tic <- Sys.time()
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .options.mpi=list(seed=95886868,chunkSize=1),
          .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnWonkCompDest,
           method="Nelder-Mead",
           start=as.list(start[c("rho","theta","delta")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=4000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
  
  toc <- Sys.time()
  print(toc-tic)
  
  results
}) -> results
```

```{r wonk-compdest-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
  extract(!sapply(results,inherits,"try-error")) %>%
  ldply() 

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
             fill=ifelse(loglik>max(loglik)-2000,
                         loglik,NA)))+
    geom_tile(color=NA)+geom_contour(bins=100,color='white')+
    labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))

results %>%
    subset(loglik==max(loglik)) -> mle.wonkcompdest
```

```{r wonk-compdest-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+rho+delta+tau1+tau2,
      data=results,
      subset=loglik>max(loglik)-10000,cex=0.5)
```

# Stouffer model

Let $i$ be the recipient town; $j$, the donor.
Let $S(i,j)$ be the collection of towns closer to town $i$ than $j$ is.
That is, $S(i,j) = \{k: k\ne i \ \&\ d(i,k) \le d(i,j)\}$.

$$\iota_i = N_i^{\tau_1}\,\left(\theta\,\sum_j\!\left(\frac{N_j}{\sum_{k\in S(i,j)}{N_k}}\right)^{\tau_2}\,\frac{I_j}{N_j}+\phi\right).$$


```{r rankmat,cache=FALSE}
bake(file="rankmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/(sum(N[distances[i,]<=distances[i,j]])-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

The negative log likelihood function for the @xia2004measles model is:

```{r stouffer-like}
likfnStouffer <- function (theta, phi, tau1, tau2, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*((rr^tau2)%*%ylag)+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

We compute the profile likelihood as before.

```{r stouffer-profile-2D,cache=FALSE}
bake(file="stouffer.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.5, 1.2, length=25),
    tau2=seq(0.5, 2.0, length=25),
    theta=log(0.2),
    phi=log(0.0001),
    psi=log(5)
  )
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnStouffer,
           method="Nelder-Mead",
           start=as.list(start[c("theta","phi","psi")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=10000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
    
  attr(results,"nproc") <- getDoParWorkers()

  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r stouffer-mle,purl=FALSE,cache=FALSE}
bake("stouffer-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnStouffer,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.stouffer
```

```{r stouffer-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(is.finite(loglik))

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.stouffer)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.stouffer,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.stouffer,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r stouffer-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+psi+tau1+tau2,
      data=results,
      subset=loglik>max(loglik,na.rm=TRUE)-10000,cex=0.5)
```

## Variant: Stouffer model with recipient included

Let $i$ be the recipient town; $j$, the donor.
Let $S'(i,j)$ be the collection of towns closer to town $i$ than $j$ is.
That is, $S'(i,j) = \{k: d(i,k) \le d(i,j)\}$.
Note that $S'(i,j)$ includes $i$, whilst $S(i,j)$ does not. 

$$\iota_i = N_i^{\tau_1}\,\left(\theta\,\sum_j\!\left(\frac{N_j}{\sum_{k\in S'(i,j)}{N_k}}\right)^{\tau_2}\,\frac{I_j}{N_j}+\phi\right)$$


```{r rankmat1,cache=FALSE}
bake(file="rankmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/sum(N[distances[i,]<=distances[i,j]])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r stouffer1-like}
likfnStouffer1 <- function (theta, phi, tau1, tau2, psi) {
  theta <- exp(theta)
  phi <- exp(phi)
  iota <- (N^tau1)*(theta*(rr^tau2)%*%ylag+phi)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dnbinom(x=obs,mu=lambda,size=exp(-psi),log=TRUE))
}
```

The profile likelihood computation.

```{r stouffer1-profile-2D,cache=FALSE}
bake(file="stouffer1.rds",{
  
  grd <- expand.grid(
    tau1=seq(0.5, 1.2, length=25),
    tau2=seq(0.5, 2.0, length=25),
    theta=log(0.2),
    phi=log(0.0001),
    psi=log(5)
  )
  
  foreach(start=iter(grd,"row"),
          .errorhandling="pass",
          .inorder=FALSE,
          .packages="bbmle"
  ) %dopar% try(
    {
      mle2(likfnStouffer1,
           method="Nelder-Mead",
           start=as.list(start[c("theta","phi","psi")]),
           fixed=as.list(start[c("tau1","tau2")]), 
           control=list(trace=0,maxit=10000),
           skip.hessian=TRUE) -> fit
      c(coef(fit),loglik=as.numeric(logLik(fit)),conv=fit@details$convergence)
    }
  ) -> results
    
  attr(results,"nproc") <- getDoParWorkers()

  results
}) -> results
```

The above was completed in `r round(attr(results,"system.time")[3]/60,1)`&nbsp;min on a `r attr(results,"nproc")`-core cluster. 

```{r stouffer1-mle,purl=FALSE,cache=FALSE}
bake("stouffer1-mle.rds",{
  
  results %>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(loglik==max(loglik),select=-c(loglik,conv)) -> start
  
  mle2(likfnStouffer1,
    method="Nelder-Mead",
    start=as.list(start),
    control=list(trace=0,maxit=10000),
    skip.hessian=TRUE) -> fit
  
  c(coef(fit),loglik=as.numeric(logLik(fit)),
    conv=fit@details$convergence) %>%
    as.list() %>%
    as.data.frame() -> mle
}) -> mle.stouffer1
```

```{r stouffer1-results,purl=FALSE}
results %>%
  extract(sapply(results,inherits,"try-error")) %>%
  sapply(as.character) %>%
  unique()

results %<>%
    extract(!sapply(results,inherits,"try-error")) %>%
    ldply() %>%
    subset(is.finite(loglik))

results %>%
  ggplot(aes(x=tau1,y=tau2,z=loglik,
    fill=ifelse(loglik>max(loglik)-2000,loglik,NA)))+
  geom_tile(color=NA)+geom_contour(bins=100,color='white')+
  geom_point(color='red',shape="x",size=3,data=mle.stouffer1)+
  geom_hline(color='red',size=0.2,linetype=2,data=mle.stouffer1,aes(yintercept=tau2))+
  geom_vline(color='red',size=0.2,linetype=2,data=mle.stouffer1,aes(xintercept=tau1))+
  labs(x=expression(tau[1]),y=expression(tau[2]),fill=expression(log(L)))
```

```{r stouffer1-pairsplot,fig.height=6.83,purl=FALSE}
pairs(~loglik+theta+phi+psi+tau1+tau2,
      data=results,
      subset=loglik>max(loglik,na.rm=TRUE)-10000,cex=0.5)
```


# Radiation model

$$\iota_i=\theta \sum_j N_j \frac{N_j N_i}{(N_j + \sum_{k \in S(i,j)} N_k)(N_j + N_i + \sum_{k \in S(i,j)} N_k)} \frac{I_j}{N_j}$$

```{r radmat,cache=FALSE}
bake(file="radmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/s/(s-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r radiation-like}
likfnRadiation <- function (theta) {
  theta <- exp(theta)
  iota <- theta*(rr%*%ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

```{r radiation-fit,purl=FALSE}
par <- list(theta=0)
with(par,likfnRadiation(theta))

bake(file="radiation.rds",{
  mle2(likfnRadiation,
       method="Brent",
       lower=-20,upper=20,
       start=par,
       control=list(trace=0,maxit=4000))
}) -> fit

summary(fit)

with(as.list(coef(fit)),c(exp(c(theta=theta))))

exp(confint(fit))
```

```{r radiation-results,purl=FALSE,include=FALSE}
c(coef(fit),loglik=logLik(fit)) %>%
  as.list() %>% as.data.frame() -> mle.radiation
```

## Variant: radiation model with recipient included

$$\iota_i=\theta \sum_j N_j \frac{N_j N_i}{(N_j + \sum_{k \in S'(i,j)} N_k)(N_j + N_i + \sum_{k \in S'(i,j)} N_k)} \frac{I_j}{N_j}$$

```{r radmat1,cache=FALSE}
bake(file="radmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/(s+N[i])/(s)
    }
  }
  diag(rr) <- 0
  rr
}) -> rr
```

```{r radiation1-like}
likfnRadiation1 <- function (theta) {
  theta <- exp(theta)
  iota <- theta*(rr%*%ylag)
  lambda <- betaS*iota[relevant]^alpha
  -sum(dpois(x=obs,lambda=lambda,log=TRUE))
}
```

```{r radiation1-fit,purl=FALSE}
par <- list(theta=0)
with(par,likfnRadiation1(theta))

bake(file="radiation1.rds",{
  mle2(likfnRadiation1,
       method="Brent",
       lower=-20,upper=20,
       start=par,
       control=list(trace=0,maxit=4000))
}) -> fit

summary(fit)

with(as.list(coef(fit)),c(exp(c(theta=theta))))

exp(confint(fit))
```

```{r radiation1-results,purl=FALSE,include=FALSE}
c(coef(fit),loglik=logLik(fit)) %>%
  as.list() %>% as.data.frame() -> mle.radiation1
```


# Model comparison

```{r modelcomp,cache=FALSE,echo=FALSE,purl=FALSE}
list(gravity=mle.grav,
     xia=mle.xia,
     comp.dest=mle.compdest,
     wonky.comp.dest=mle.wonkcompdest,
     stouffer=mle.stouffer,
     stouffer.variant=mle.stouffer1,
     radiation=mle.radiation,
     radiation.variant=mle.radiation1) %>%
  ldply(.id="model") %>%
  arrange(-loglik) %>%
  mutate(rho=exp(rho),psi=exp(psi)) %>%
  rename(c(theta="$\\log{\\theta}$",phi="$\\log{\\phi}$",rho="$\\rho$",
    psi="$\\psi$",tau1="$\\tau_1$",tau2="$\\tau_2$",delta="$\\delta$",
    loglik="$\\ell$")) %>%
  kable(digits=3)
```

# Diagnostics

To study whether better statistical fit translates to meaningful improvement on prediction on incidence we can look at CCS patterns:

```
dat$E=dat$cases==0 
dat %>% ddply(~town,summarize,mean(E)) -> ext
plot(ext[,2]~N)
```

We can predict importation rates from fitted models

```{r iota-plot,fig.height=6.83,purl=FALSE,eval=FALSE}
#Gravity
readRDS("gravity-mle.rds") -> test

cfs=with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),tau1=tau1,tau2=tau2))

Q<-with(as.list(cfs),
 (N^tau2)*(dd^rho)
)

iotagr<-apply(
with(as.list(cfs),
  theta*(N^tau1)*crossprod(Q,ylag)
),1,mean)

plot(iotagr~N, log="xy", ylab="imports")

#Xia
readRDS("xia-test.rds") -> test

summary(test)

cfs<-with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),tau1=tau1,tau2=tau2))
 Q <- with(as.list(cfs),
 (N^tau2)*(dd^rho))

iotaxi<-apply(
with(as.list(cfs),
  theta*(N^tau1)*crossprod(Q,ylag^tau2)
),1,mean)

points(iotaxi~N, col=2)

#CD
bake(file="compdest-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnCompDest,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=4000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test

cfs<-with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),delta=delta,tau1=tau1,tau2=tau2))
 Q <- with(as.list(cfs),
 (N^tau2)*(dd^rho))

iii <- 1-diag(length(N))

 R <- with(as.list(cfs),
 crossprod(Q,iii)^delta)

iotacd<-apply(
with(as.list(cfs),
theta*(N^tau1)*crossprod(Q*R,ylag)
),1,mean)

points(iotacd~N, col=3)

#WCD
bake(file="wonkcompdest-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnWonkCompDest,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=4000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test

cfs<-with(as.list(coef(test)),c(exp(c(theta=theta,rho=rho)),delta=delta,tau1=tau1,tau2=tau2))
 Q <- with(as.list(cfs),
 (N^tau2)*(dd^rho))

iii <- 1-diag(length(N))

 R <- with(as.list(cfs),
crossprod(Q,iii)^delta)

iotawc<-apply(
with(as.list(cfs),
theta*(N^tau1)*crossprod(Q*R,ylag^tau2)
),1,mean)

points(iotawc~N, col=4)

#Stouffer
bake(file="rankmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/(sum(N[distances[i,]<=distances[i,j]])-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr

bake(file="stouffer-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnStouffer,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=4000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test


cfs<-with(as.list(coef(test)),c(exp(c(theta=theta)),tau1=tau1,tau2=tau2))

iotast <-apply(
with(as.list(cfs), theta*(N^tau1)*((rr^tau2)%*%ylag)
),1,mean)
points(iotast~N, col=5)



#Stouffer variant
bake(file="rankmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      rr[i,j] <- N[j]/sum(N[distances[i,]<=distances[i,j]])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr

bake(file="stouffer1-test.rds",{
    tic <- Sys.time()
    test <- mle2(likfnStouffer1,
                 method="Nelder-Mead",
                 start=par,
                 control=list(trace=0,maxit=4000))
    toc <- Sys.time()
    toc-tic
    test
}) -> test


cfs<-with(as.list(coef(test)),c(exp(c(theta=theta)), tau1=tau1,tau2=tau2))

iotavs <-apply(
with(as.list(cfs), theta*(N^tau1)*((rr^tau2)%*%ylag)
),1,mean)
points(iotavs~N, col=5)

#Radiation
bake(file="radmat.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/s/(s-N[i])
    }
  }
  diag(rr) <- 0
  rr
}) -> rr

bake(file="radiation.rds",{
  mle2(likfnRadiation,
       method="Brent",
       lower=-20,upper=20,
       start=par,
       control=list(trace=0,maxit=4000))
}) -> fit

cfs=with(as.list(coef(fit)),c(exp(c(theta=theta))))
iotara <-apply(
with(as.list(cfs), theta*(rr%*%ylag)
),1,mean)

#Radiation Variant
bake(file="radmat1.rds",{
  rr <- array(dim=dim(distances),dimnames=dimnames(distances))
  for (i in seq_along(N)) {
    for (j in seq_along(N)) {
      s <- sum(N[distances[i,]<=distances[i,j]])
      rr[i,j] <- N[i]*N[j]*N[j]/(s+N[i])/(s)
    }
  }
  diag(rr) <- 0
  rr
}) -> rr

bake(file="radiation1.rds",{
  mle2(likfnRadiation1,
       method="Brent",
       lower=-20,upper=20,
       start=par,
       control=list(trace=0,maxit=4000))

}) -> test

cfs<-with(as.list(coef(test)),c(exp(c(theta=theta))))
iotavr <-apply(
with(as.list(cfs), theta*(rr%*%ylag)
),1,mean)

points(iotavr~N, col=6)
legend("topleft",
       legend=c("Gravity", "Xia", "Compdest",
                "X Compdest", "V Stouffer", "V Radiat"),
       pch=rep(21,6),
        col=1:6)

```

We next create the fade-out plot
```{r fadeout-plot,fig.height=6.83,purl=FALSE}
dat$E=dat$cases==0 
dat %>% ddply(~town,summarize,mean(E)) -> ext

par(mfrow=c(1,1))
par(mar = c(5,5,2,5))
plot(ext[,2]~N, log="x", xlab="Pop.size", ylab="Presence")

par(new=T)

plot(log(ext[,2]/(1-ext[,2]))~N, log="x",type="p", col="red", axes=FALSE, xlab=NA, ylab=NA)
axis(side = 4)
mtext(side = 4, line = 4, "logit")
legend("topright",
       legend=c("Presence", "logit"),
       pch=c(21,21),
        col=c("black", "red"))
```
And do spatial plot of residuals 
```{r residual-plot,fig.height=6.83,purl=FALSE,eval=F}
library(ncf)
dfr=data.frame(lon=latlon$Long, lat=latlon$Lat, gr=iotagr, cd=iotacd, res=iotagr-iotacd)
spatial.plot(dfr$lon, dfr$lat, edat$resE, inches=.1)
title("excess fade-out")
```
Then generate deviations ffrom expectation based on size for all of the spatial models.
```{r expect-spatial,purl=FALSE,eval=F}
edat=data.frame(N=N, E=ext, logitE=log(ext[,2]/(1-ext[,2])), 
gr=iotagr,
xi=iotaxi,
cd=iotacd,
wc=iotawc,
st=iotast,
vs=iotavs,
ra=iotara,
vr=iotavr
)
```

And create residuals of all against log(N)

```{r residuals,purl=FALSE,eval=F}
edat$logitE[edat$logitE==-Inf]=NA
edat$resE=NA
edat$resE[!is.na(edat$logitE)]=resid(lm(logitE~I(log(N)), data=edat, na.action=na.omit))
edat%<>%mutate(resGR=resid(lm(log(gr)~I(log(N)))))
edat%<>%mutate(resXI=resid(lm(log(xi)~I(log(N)))))
edat%<>%mutate(resCD=resid(lm(log(cd)~I(log(N)))))
edat%<>%mutate(resWC=resid(lm(log(wc)~I(log(N)))))
edat%<>%mutate(resST=resid(lm(log(st)~I(log(N)))))
edat%<>%mutate(resVS=resid(lm(log(vs)~I(log(N)))))
edat%<>%mutate(resRA=resid(lm(log(ra)~I(log(N)))))
edat%<>%mutate(resVR=resid(lm(log(vr)~I(log(N)))))
```
The correlation among residuals
```{r cormat,purl=FALSE,eval=F}
round(cor(edat[,13:21], use="pairwise.complete"), 3)
```
Clearly the Comp-Dest versions are best.

```{r corr plot,purl=FALSE,eval=F}
cc=matrix(NA, ncol=3, nrow=8)
for(i in 1:8){
  cc[i,]=cor.test(-edat[,13], edat[,13+i])$estimate
  cc[i,2:3]=cor.test(-edat[,13], edat[,13+i])$conf.int[1:2]
}
cc=as.data.frame(cc)
dimnames(cc)=list(c("GR", "XI", "CD", "xC", "ST", "SV", "RA", "RV"), c("Est", "L", "U"))

library(plotrix)
plotCI(cc[,1], ui=cc[,3], li=cc[,2], ylab="cor", xlab="")
#text(1:8,0,c("GR", "XI", "CD", "xC", "ST", "SV", "RA", "RV"))
axis(1, at=1:8, labels=c("GR", "XI", "CD", "xC", "ST", "SV", "RA", "RV"))
```
Plot of difference between Xia and xCD
```{r difference plot,purl=FALSE,eval=F}
spatial.plot(dfr$lon, dfr$lat, log(edat$wc)-log(edat$xi), inches=.1)
title("xCD vs Xia difference")
```
It may also be of interest to take a clustering approach to comparing model predictions...

```{r k-means-dendro,purl=FALSE,eval=F}
hc = hclust(dist(t(edat[,14:21])))
plot(hc)
```

# Next steps

1. Debug loglikelihood for Xia and gravity models
2. Include intercept term to capture background
1. Use negative binomial model
3. Compare with "null" models (diffusion, independent)
4. Model-model comparison of coupling matrices (which places are the strongest outliers) using likelihood residuals

Plots:

1. empirical CCS plot with residuals (how well do models capture the residuals?)
1. likelihood comparison maps

1. Expontial distance kernels
1. qAICs and qBayes-weights in table 
1. Solve optimization issues
1. Random effects.
1. Zero-inflated or negative binomial model?
1. Revisit $\sigma$ profile
1. Revisit TSIR fitting (include residuals?)
1. Include references for the various models.

```{r stop-mpi,include=FALSE}
if (mpi) {
    closeCluster(cl)
    try(detach("package:doMPI",unload=TRUE),silent=TRUE)
    mpi.exit()
    try(detach("package:Rmpi",unload=TRUE),silent=TRUE)
}
```

# References
